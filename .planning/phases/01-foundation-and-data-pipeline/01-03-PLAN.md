---
phase: 01-foundation-and-data-pipeline
plan: "03"
type: execute
wave: 3
depends_on: ["01-01", "01-02"]
files_modified:
  - src/classifier_training/data/__init__.py
  - src/classifier_training/data/dataset.py
  - src/classifier_training/data/datamodule.py
  - tests/test_dataset.py
  - tests/test_datamodule.py
autonomous: true

must_haves:
  truths:
    - "`from classifier_training.data import ImageFolderDataModule` succeeds"
    - "`DataModule.setup('fit')` on the real dataset produces train and val DataLoaders with correct lengths"
    - "Train DataLoader uses `WeightedRandomSampler` (shuffle=False, sampler provided)"
    - "Val and test DataLoaders have `shuffle=False` and no sampler — deterministic"
    - "Train transform pipeline contains `RandomResizedCrop` and `ColorJitter`; val/test do NOT"
    - "Two val batches from the same index are pixel-identical (no augmentation on val)"
    - "`DataModule.save_labels_mapping(path)` writes a JSON file with 43 keys in `class_to_idx`"
    - "`class_to_idx['']` equals `0` (empty-string class sorts first alphabetically)"
    - "`len(train_dataset)` equals 2930 (annotation rows, NOT unique image count of 2891)"
    - "`pixi run lint`, `pixi run typecheck`, `pixi run test` all exit 0"
  artifacts:
    - path: "src/classifier_training/data/dataset.py"
      provides: "JerseyNumberDataset — reads flat JSONL-annotated dir, builds (image_path, label_idx) list from annotation rows"
      exports: ["JerseyNumberDataset"]
    - path: "src/classifier_training/data/datamodule.py"
      provides: "ImageFolderDataModule — LightningDataModule with strict transform separation, WeightedRandomSampler, save_labels_mapping"
      exports: ["ImageFolderDataModule", "IMAGENET_MEAN", "IMAGENET_STD"]
    - path: "src/classifier_training/data/__init__.py"
      provides: "Public API re-export"
      exports: ["ImageFolderDataModule"]
    - path: "tests/test_dataset.py"
      provides: "JerseyNumberDataset unit tests"
    - path: "tests/test_datamodule.py"
      provides: "ImageFolderDataModule integration tests against real dataset"
  key_links:
    - from: "src/classifier_training/data/datamodule.py"
      to: "src/classifier_training/data/dataset.py"
      via: "instantiates JerseyNumberDataset in setup()"
      pattern: "JerseyNumberDataset\\("
    - from: "src/classifier_training/data/datamodule.py"
      to: "src/classifier_training/config.py"
      via: "accepts DataModuleConfig in __init__"
      pattern: "DataModuleConfig"
    - from: "tests/test_datamodule.py"
      to: "/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/data/basketball-jersey-numbers-ocr"
      via: "real dataset integration tests (parametrized with pytest.mark.integration)"
      pattern: "basketball-jersey-numbers-ocr"
---

<objective>
Implement `JerseyNumberDataset` and `ImageFolderDataModule` — the core data layer. The DataModule reads the flat JSONL-annotated basketball-jersey-numbers-ocr dataset, applies strictly separated transform pipelines (train gets augmentation, val/test do not), handles class imbalance with `WeightedRandomSampler`, and persists `labels_mapping.json` with alphabetically-ordered `class_to_idx`.

Purpose: This is the primary deliverable of Phase 1. All downstream phases (Model, Callbacks, Training Config) depend on this DataModule contract being correct and tested.
Output: Working `ImageFolderDataModule` that satisfies all 5 Phase 1 success criteria.
</objective>

<execution_context>
@/Users/ortizeg/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ortizeg/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-and-data-pipeline/01-RESEARCH.md
@.planning/phases/01-foundation-and-data-pipeline/01-01-SUMMARY.md
@.planning/phases/01-foundation-and-data-pipeline/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: JerseyNumberDataset and ImageFolderDataModule</name>
  <files>
    src/classifier_training/data/__init__.py
    src/classifier_training/data/dataset.py
    src/classifier_training/data/datamodule.py
  </files>
  <action>
Create `src/classifier_training/data/__init__.py`:
```python
"""Data pipeline for classifier_training."""

from classifier_training.data.datamodule import ImageFolderDataModule

__all__ = ["ImageFolderDataModule"]
```

Create `src/classifier_training/data/dataset.py`:

```python
"""JSONL-annotated image dataset for jersey number classification."""

import json
from pathlib import Path
from typing import Callable

import torch
from loguru import logger
from PIL import Image
from torch.utils.data import Dataset


class JerseyNumberDataset(Dataset[tuple[torch.Tensor, int]]):
    """Dataset for flat-directory JSONL-annotated image classification.

    The basketball-jersey-numbers-ocr dataset is NOT ImageFolder-compatible:
    images are in flat directories (no class subdirs). Each split has a single
    annotations.jsonl where each line is:
        {"image": "filename.jpg", "prefix": "Read the number.", "suffix": "<label>"}

    One annotation row = one training sample. Some images appear in multiple rows
    (different label per crop annotation). Build samples from annotation rows,
    NOT from the image file list — len(dataset) = len(annotation_rows).

    Args:
        root: Split directory containing flat image files and annotations.jsonl.
        class_to_idx: Alphabetically-ordered mapping from label string to integer.
            MUST be built from train split only and shared across val/test.
        transform: Optional callable applied to PIL Image, returns torch.Tensor.
    """

    def __init__(
        self,
        root: Path,
        class_to_idx: dict[str, int],
        transform: Callable[[Image.Image], torch.Tensor] | None = None,
    ) -> None:
        self.root = root
        self.class_to_idx = class_to_idx
        self.transform = transform
        self.samples: list[tuple[Path, int]] = []

        ann_path = root / "annotations.jsonl"
        with open(ann_path) as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                record = json.loads(line)
                img_path = root / record["image"]
                label_idx = class_to_idx[record["suffix"]]
                self.samples.append((img_path, label_idx))

        logger.debug(
            f"JerseyNumberDataset: loaded {len(self.samples)} samples from {ann_path}"
        )

    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, idx: int) -> tuple[torch.Tensor, int]:
        img_path, label = self.samples[idx]
        img = Image.open(img_path).convert("RGB")
        if self.transform is not None:
            img = self.transform(img)  # type: ignore[assignment]
        return img, label  # type: ignore[return-value]
```

Create `src/classifier_training/data/datamodule.py`:

```python
"""LightningDataModule for the basketball jersey numbers OCR dataset."""

import json
from pathlib import Path

import lightning as L
import torch
from loguru import logger
from torch.utils.data import DataLoader, WeightedRandomSampler
from torchvision.transforms import v2

from classifier_training.config import DataModuleConfig
from classifier_training.data.dataset import JerseyNumberDataset

# ImageNet normalization statistics — stored here and written to labels_mapping.json.
# The ONNX inference pipeline in basketball-2d-to-3d reads normalization params
# from labels_mapping.json and applies them at inference time.
# Do NOT bake normalization into the ONNX graph (user decision, per STATE.md).
IMAGENET_MEAN: list[float] = [0.485, 0.456, 0.406]
IMAGENET_STD: list[float] = [0.229, 0.224, 0.225]


class ImageFolderDataModule(L.LightningDataModule):
    """DataModule for the basketball jersey numbers OCR dataset.

    Reads flat JSONL-annotated splits from data_root/{train,valid,test}/.
    Applies strictly separate transform pipelines:
      - Train: RandomResizedCrop + RandomHorizontalFlip + ColorJitter + Normalize
      - Val/Test: Resize(256) + CenterCrop + Normalize (deterministic only)

    class_to_idx is built from train split only, sorted alphabetically. The empty-string
    class ("") is a legitimate class at index 0. All 43 classes must be in the mapping.

    Args:
        config: DataModuleConfig frozen model with all DataLoader parameters.
    """

    def __init__(self, config: DataModuleConfig) -> None:
        super().__init__()
        self._config = config
        self._data_root = Path(config.data_root)

        # Apply MPS guard: multiprocessing DataLoader workers crash on Apple Silicon MPS.
        # Detect at construction time and override num_workers.
        # Per research/sibling pattern — must set num_workers=0 on MPS.
        num_workers = config.num_workers
        if torch.backends.mps.is_available() and num_workers > 0:
            logger.warning(
                "MPS detected: setting num_workers=0 to avoid multiprocessing crash. "
                "Use linux-64 / CUDA for multi-worker DataLoading."
            )
            num_workers = 0

        self._num_workers = num_workers
        self._pin_memory = config.pin_memory
        # persistent_workers is meaningless (and silently ignored) with num_workers=0
        self._persistent_workers = config.persistent_workers and num_workers > 0
        self._batch_size = config.batch_size
        self._image_size = config.image_size

        self._class_to_idx: dict[str, int] | None = None
        self._train_dataset: JerseyNumberDataset | None = None
        self._val_dataset: JerseyNumberDataset | None = None
        self._test_dataset: JerseyNumberDataset | None = None

    # ------------------------------------------------------------------
    # Class mapping — built from train only, alphabetical, deterministic
    # ------------------------------------------------------------------

    @property
    def class_to_idx(self) -> dict[str, int]:
        """Alphabetically-sorted class_to_idx built from train annotations.

        Built lazily on first access. Safe to call before setup().
        """
        if self._class_to_idx is None:
            self._build_class_to_idx()
        return self._class_to_idx  # type: ignore[return-value]

    @property
    def num_classes(self) -> int:
        """Total number of classes (43 for basketball jersey numbers dataset)."""
        return len(self.class_to_idx)

    def _build_class_to_idx(self) -> None:
        """Build alphabetically-sorted class_to_idx from train split annotations.

        CRITICAL invariants:
        - Built from train only (val/test may not cover all classes).
        - Alphabetical sort: "" < "0" < "00" < "1" < "10" < ... (lexicographic, not numeric).
        - "" (empty string) gets index 0 — represents unreadable jersey numbers.
        - Must have exactly 43 classes for basketball jersey numbers dataset.
        """
        ann_path = self._data_root / "train" / "annotations.jsonl"
        classes: set[str] = set()
        with open(ann_path) as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                classes.add(json.loads(line)["suffix"])
        self._class_to_idx = {cls: i for i, cls in enumerate(sorted(classes))}
        logger.info(
            f"Built class_to_idx: {len(self._class_to_idx)} classes from {ann_path}"
        )
        logger.debug(f"class_to_idx: {self._class_to_idx}")

    # ------------------------------------------------------------------
    # Transform pipelines — strictly separated at construction time
    # ------------------------------------------------------------------

    def _build_train_transforms(self) -> v2.Compose:
        """Train transforms: augmentation + normalization.

        RandomResizedCrop and ColorJitter are ONLY applied to the train dataset.
        These transforms are assigned at Dataset construction time, not conditionally
        in __getitem__ — this is the only safe pattern (DATA-03).
        """
        return v2.Compose([
            v2.RandomResizedCrop(self._image_size),
            v2.RandomHorizontalFlip(),
            v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            v2.ToImage(),
            v2.ToDtype(torch.float32, scale=True),
            v2.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
        ])

    def _build_val_test_transforms(self) -> v2.Compose:
        """Val/test transforms: deterministic resize + normalize only.

        No augmentation. Two passes over the same image produce identical tensors.
        Uses v2.ToImage() + v2.ToDtype() — ToTensor() is deprecated in v2.
        """
        return v2.Compose([
            v2.Resize(256),
            v2.CenterCrop(self._image_size),
            v2.ToImage(),
            v2.ToDtype(torch.float32, scale=True),
            v2.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
        ])

    # ------------------------------------------------------------------
    # LightningDataModule lifecycle
    # ------------------------------------------------------------------

    def setup(self, stage: str | None = None) -> None:
        """Instantiate datasets for the given stage.

        Args:
            stage: "fit", "test", or None (all stages).
                   "fit" instantiates train + val.
                   "test" instantiates test only.
                   None instantiates all three.
        """
        if stage in ("fit", None):
            self._train_dataset = JerseyNumberDataset(
                root=self._data_root / "train",
                class_to_idx=self.class_to_idx,
                transform=self._build_train_transforms(),
            )
            self._val_dataset = JerseyNumberDataset(
                root=self._data_root / "valid",
                class_to_idx=self.class_to_idx,
                transform=self._build_val_test_transforms(),
            )
            logger.info(
                f"Setup fit: train={len(self._train_dataset)}, "
                f"val={len(self._val_dataset)} samples"
            )

        if stage in ("test", None):
            self._test_dataset = JerseyNumberDataset(
                root=self._data_root / "test",
                class_to_idx=self.class_to_idx,
                transform=self._build_val_test_transforms(),
            )
            logger.info(f"Setup test: {len(self._test_dataset)} samples")

    # ------------------------------------------------------------------
    # Class imbalance handling
    # ------------------------------------------------------------------

    def _compute_class_weights(self) -> torch.Tensor:
        """Inverse-frequency class weight tensor for weighted CrossEntropyLoss.

        Returns tensor of shape (num_classes,) where rare classes have higher weight.
        Normalized so weights sum to num_classes (preserves scale w.r.t. unweighted loss).
        """
        assert self._train_dataset is not None, "Call setup('fit') before _compute_class_weights"
        counts = torch.zeros(self.num_classes)
        for _, label in self._train_dataset.samples:
            counts[label] += 1.0
        weights = 1.0 / counts.clamp(min=1.0)
        return weights / weights.sum() * self.num_classes

    def get_class_weights(self) -> torch.Tensor:
        """Public accessor for class weights tensor (for weighted CrossEntropyLoss).

        Requires setup('fit') to have been called first.
        """
        return self._compute_class_weights()

    def _build_sampler(self) -> WeightedRandomSampler:
        """Per-sample weight sampler for WeightedRandomSampler.

        Uses inverse class frequency: rare classes get higher per-sample weight.
        replacement=True required for WeightedRandomSampler (samples may repeat per epoch).

        IMPORTANT: DataLoader using this sampler MUST set shuffle=False.
        shuffle=True + sampler raises ValueError in PyTorch DataLoader.
        """
        assert self._train_dataset is not None, "Call setup('fit') before _build_sampler"
        class_weights = self._compute_class_weights()
        sample_weights = [
            class_weights[label].item()
            for _, label in self._train_dataset.samples
        ]
        return WeightedRandomSampler(
            weights=sample_weights,
            num_samples=len(sample_weights),
            replacement=True,
        )

    # ------------------------------------------------------------------
    # DataLoaders
    # ------------------------------------------------------------------

    def train_dataloader(self) -> DataLoader[tuple[torch.Tensor, int]]:
        assert self._train_dataset is not None, "Call setup('fit') first"
        sampler = self._build_sampler()
        return DataLoader(
            self._train_dataset,
            batch_size=self._batch_size,
            sampler=sampler,
            shuffle=False,  # MUST be False when sampler is provided — mutually exclusive
            num_workers=self._num_workers,
            pin_memory=self._pin_memory,
            persistent_workers=self._persistent_workers,
        )

    def val_dataloader(self) -> DataLoader[tuple[torch.Tensor, int]]:
        assert self._val_dataset is not None, "Call setup('fit') first"
        return DataLoader(
            self._val_dataset,
            batch_size=self._batch_size,
            shuffle=False,
            num_workers=self._num_workers,
            pin_memory=self._pin_memory,
            persistent_workers=self._persistent_workers,
        )

    def test_dataloader(self) -> DataLoader[tuple[torch.Tensor, int]]:
        assert self._test_dataset is not None, "Call setup('test') first"
        return DataLoader(
            self._test_dataset,
            batch_size=self._batch_size,
            shuffle=False,
            num_workers=self._num_workers,
            pin_memory=self._pin_memory,
            persistent_workers=self._persistent_workers,
        )

    # ------------------------------------------------------------------
    # labels_mapping.json serialization
    # ------------------------------------------------------------------

    def save_labels_mapping(self, save_path: Path) -> None:
        """Persist class_to_idx and normalization stats as labels_mapping.json.

        Written alongside ONNX model exports so the basketball-2d-to-3d inference
        pipeline can apply the correct preprocessing and map logits to class labels.

        Normalization is documented here (not baked into ONNX graph) per user decision.

        Args:
            save_path: Destination path for labels_mapping.json.
        """
        mapping = {
            "num_classes": self.num_classes,
            "class_to_idx": self.class_to_idx,
            "idx_to_class": {str(v): k for k, v in self.class_to_idx.items()},
            "normalization": {
                "mean": IMAGENET_MEAN,
                "std": IMAGENET_STD,
            },
        }
        save_path.parent.mkdir(parents=True, exist_ok=True)
        with open(save_path, "w") as f:
            json.dump(mapping, f, indent=2)
        logger.info(f"Saved labels_mapping.json to {save_path}")
```
  </action>
  <verify>
    ```bash
    cd "/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/code/classifier-training"
    pixi run python -c "from classifier_training.data import ImageFolderDataModule; print('import OK')"
    pixi run lint
    pixi run typecheck
    ```
    All three must exit 0.
  </verify>
  <done>
    - `from classifier_training.data import ImageFolderDataModule` succeeds
    - `pixi run lint` exits 0
    - `pixi run typecheck` exits 0
    - All source files created with no syntax or type errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Dataset and DataModule test suites</name>
  <files>
    tests/test_dataset.py
    tests/test_datamodule.py
  </files>
  <action>
Create `tests/test_dataset.py`. Uses the `tmp_dataset_dir` fixture from conftest.py:

```python
"""Unit tests for JerseyNumberDataset."""

from pathlib import Path

import pytest
import torch

from classifier_training.data.dataset import JerseyNumberDataset


@pytest.fixture()
def class_to_idx() -> dict[str, int]:
    return {"0": 0, "1": 1, "2": 2}


class TestJerseyNumberDataset:
    def test_len_equals_annotation_rows_not_image_count(
        self, tmp_dataset_dir: Path, class_to_idx: dict[str, int]
    ) -> None:
        """len(dataset) must equal annotation rows, not unique image count.

        The real dataset has 2930 annotation rows but only 2891 unique images.
        Our fixture: 3 classes x 2 images/class = 6 annotation rows per split.
        """
        ds = JerseyNumberDataset(tmp_dataset_dir / "train", class_to_idx)
        # conftest creates 3 classes * 2 images each = 6 annotation rows
        assert len(ds) == 6

    def test_getitem_returns_tensor_and_int(
        self, tmp_dataset_dir: Path, class_to_idx: dict[str, int]
    ) -> None:
        ds = JerseyNumberDataset(tmp_dataset_dir / "train", class_to_idx)
        item = ds[0]
        assert isinstance(item, tuple)
        assert len(item) == 2
        # Without transform, returns PIL Image — but we confirm it's not None
        img, label = item
        assert isinstance(label, int)

    def test_getitem_with_transform_returns_tensor(
        self, tmp_dataset_dir: Path, class_to_idx: dict[str, int]
    ) -> None:
        from torchvision.transforms import v2

        transform = v2.Compose([
            v2.ToImage(),
            v2.ToDtype(torch.float32, scale=True),
        ])
        ds = JerseyNumberDataset(tmp_dataset_dir / "train", class_to_idx, transform=transform)
        img, label = ds[0]
        assert isinstance(img, torch.Tensor)
        assert img.shape == (3, 224, 224)
        assert isinstance(label, int)

    def test_label_indices_within_range(
        self, tmp_dataset_dir: Path, class_to_idx: dict[str, int]
    ) -> None:
        ds = JerseyNumberDataset(tmp_dataset_dir / "train", class_to_idx)
        for _, label in ds.samples:
            assert 0 <= label < len(class_to_idx)

    def test_all_splits_load(
        self, tmp_dataset_dir: Path, class_to_idx: dict[str, int]
    ) -> None:
        for split in ("train", "valid", "test"):
            ds = JerseyNumberDataset(tmp_dataset_dir / split, class_to_idx)
            assert len(ds) == 6  # 3 classes * 2 images per conftest fixture

    def test_class_to_idx_stored_on_instance(
        self, tmp_dataset_dir: Path, class_to_idx: dict[str, int]
    ) -> None:
        ds = JerseyNumberDataset(tmp_dataset_dir / "train", class_to_idx)
        assert ds.class_to_idx == class_to_idx
```

Create `tests/test_datamodule.py`. Two test classes: one uses synthetic fixture (fast), one uses the real dataset (marked `integration`):

```python
"""Tests for ImageFolderDataModule."""

import json
from pathlib import Path

import pytest
import torch
from torch.utils.data import WeightedRandomSampler

from classifier_training.config import DataModuleConfig
from classifier_training.data import ImageFolderDataModule
from classifier_training.data.datamodule import IMAGENET_MEAN, IMAGENET_STD

REAL_DATASET = Path(
    "/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/data/basketball-jersey-numbers-ocr"
)


# ---------------------------------------------------------------------------
# Synthetic dataset tests (fast, no real data dependency)
# ---------------------------------------------------------------------------


class TestImageFolderDataModuleSynthetic:
    def test_import_succeeds(self) -> None:
        """Phase 1 success criterion 1: importable without error."""
        from classifier_training.data import ImageFolderDataModule as DM  # noqa: F401

        assert DM is not None

    def test_setup_fit_creates_train_and_val_datasets(
        self, tmp_dataset_dir: Path
    ) -> None:
        cfg = DataModuleConfig(data_root=str(tmp_dataset_dir), num_workers=0)
        dm = ImageFolderDataModule(cfg)
        dm.setup("fit")
        assert dm._train_dataset is not None
        assert dm._val_dataset is not None
        assert dm._test_dataset is None  # not setup for test stage

    def test_setup_test_creates_test_dataset(self, tmp_dataset_dir: Path) -> None:
        cfg = DataModuleConfig(data_root=str(tmp_dataset_dir), num_workers=0)
        dm = ImageFolderDataModule(cfg)
        dm.setup("test")
        assert dm._test_dataset is not None
        assert dm._train_dataset is None

    def test_train_dataloader_uses_weighted_sampler_not_shuffle(
        self, tmp_dataset_dir: Path
    ) -> None:
        """WeightedRandomSampler must be used; shuffle must be False.

        Setting shuffle=True alongside a sampler raises ValueError in PyTorch.
        """
        cfg = DataModuleConfig(data_root=str(tmp_dataset_dir), num_workers=0)
        dm = ImageFolderDataModule(cfg)
        dm.setup("fit")
        loader = dm.train_dataloader()
        assert loader.sampler is not None
        assert isinstance(loader.sampler, WeightedRandomSampler)
        # DataLoader.shuffle is not an attribute — verify via batch_sampler
        assert not isinstance(loader.batch_sampler, torch.utils.data.RandomSampler)

    def test_val_dataloader_has_no_sampler(self, tmp_dataset_dir: Path) -> None:
        cfg = DataModuleConfig(data_root=str(tmp_dataset_dir), num_workers=0)
        dm = ImageFolderDataModule(cfg)
        dm.setup("fit")
        loader = dm.val_dataloader()
        assert isinstance(loader.sampler, torch.utils.data.SequentialSampler)

    def test_val_batch_is_deterministic(self, tmp_dataset_dir: Path) -> None:
        """Val transforms must not include augmentation.

        Two forward passes over the same val sample must produce identical tensors.
        This verifies DATA-03: strict transform separation.
        """
        cfg = DataModuleConfig(data_root=str(tmp_dataset_dir), num_workers=0)
        dm = ImageFolderDataModule(cfg)
        dm.setup("fit")
        assert dm._val_dataset is not None
        img1, lbl1 = dm._val_dataset[0]
        img2, lbl2 = dm._val_dataset[0]
        assert isinstance(img1, torch.Tensor)
        assert torch.allclose(img1, img2), "Val transform is not deterministic — augmentation leak"
        assert lbl1 == lbl2

    def test_class_to_idx_built_from_train_only(self, tmp_dataset_dir: Path) -> None:
        """class_to_idx must be derived from train split, not val/test."""
        cfg = DataModuleConfig(data_root=str(tmp_dataset_dir), num_workers=0)
        dm = ImageFolderDataModule(cfg)
        # Access before setup() — lazy build from train annotations
        c2i = dm.class_to_idx
        # conftest fixture has 3 classes: "0", "1", "2"
        assert c2i == {"0": 0, "1": 1, "2": 2}

    def test_class_to_idx_alphabetically_sorted(self, tmp_dataset_dir: Path) -> None:
        cfg = DataModuleConfig(data_root=str(tmp_dataset_dir), num_workers=0)
        dm = ImageFolderDataModule(cfg)
        c2i = dm.class_to_idx
        keys = list(c2i.keys())
        assert keys == sorted(keys), "class_to_idx keys must be alphabetically sorted"
        indices = list(c2i.values())
        assert indices == list(range(len(indices))), "Indices must be 0..N-1"

    def test_save_labels_mapping_writes_json(
        self, tmp_dataset_dir: Path, tmp_path: Path
    ) -> None:
        cfg = DataModuleConfig(data_root=str(tmp_dataset_dir), num_workers=0)
        dm = ImageFolderDataModule(cfg)
        out_path = tmp_path / "exports" / "labels_mapping.json"
        dm.save_labels_mapping(out_path)
        assert out_path.exists()
        with open(out_path) as f:
            data = json.load(f)
        assert "class_to_idx" in data
        assert "idx_to_class" in data
        assert "num_classes" in data
        assert "normalization" in data
        assert data["normalization"]["mean"] == IMAGENET_MEAN
        assert data["normalization"]["std"] == IMAGENET_STD
        assert data["num_classes"] == 3  # conftest fixture has 3 classes
        assert data["class_to_idx"] == {"0": 0, "1": 1, "2": 2}

    def test_num_classes_property(self, tmp_dataset_dir: Path) -> None:
        cfg = DataModuleConfig(data_root=str(tmp_dataset_dir), num_workers=0)
        dm = ImageFolderDataModule(cfg)
        assert dm.num_classes == 3

    def test_get_class_weights_shape(self, tmp_dataset_dir: Path) -> None:
        """Class weights tensor must have shape (num_classes,)."""
        cfg = DataModuleConfig(data_root=str(tmp_dataset_dir), num_workers=0)
        dm = ImageFolderDataModule(cfg)
        dm.setup("fit")
        weights = dm.get_class_weights()
        assert weights.shape == (3,)
        assert (weights > 0).all(), "All class weights must be positive"


# ---------------------------------------------------------------------------
# Real dataset integration tests (marked integration — skipped in fast runs)
# ---------------------------------------------------------------------------


@pytest.mark.skipif(
    not REAL_DATASET.exists(),
    reason=f"Real dataset not found at {REAL_DATASET}",
)
class TestImageFolderDataModuleRealDataset:
    """Integration tests against the real basketball-jersey-numbers-ocr dataset.

    Verifies Phase 1 success criteria 4 and 5 against actual data.
    These tests are slow (disk I/O) but required for Phase 1 sign-off.
    """

    def _make_dm(self) -> ImageFolderDataModule:
        cfg = DataModuleConfig(data_root=str(REAL_DATASET), num_workers=0)
        return ImageFolderDataModule(cfg)

    def test_class_to_idx_has_43_classes(self) -> None:
        """Phase 1 success criterion 5: all 43 classes present."""
        dm = self._make_dm()
        assert dm.num_classes == 43, (
            f"Expected 43 classes, got {dm.num_classes}. "
            "Check that '' (empty string) is included."
        )

    def test_empty_string_class_at_index_zero(self) -> None:
        """'' class must sort first (index 0) — alphabetical sort invariant."""
        dm = self._make_dm()
        assert dm.class_to_idx[""] == 0, (
            "Empty-string class must be at index 0 (sorts before all numeric strings)"
        )

    def test_train_dataset_len_equals_annotation_rows(self) -> None:
        """len(train_dataset) must be 2930 (annotation rows), not 2891 (images)."""
        dm = self._make_dm()
        dm.setup("fit")
        assert dm._train_dataset is not None
        assert len(dm._train_dataset) == 2930, (
            f"Expected 2930 annotation rows, got {len(dm._train_dataset)}. "
            "Do not deduplicate by image filename."
        )

    def test_val_dataset_len_equals_annotation_rows(self) -> None:
        dm = self._make_dm()
        dm.setup("fit")
        assert dm._val_dataset is not None
        assert len(dm._val_dataset) == 372

    def test_test_dataset_len_equals_annotation_rows(self) -> None:
        dm = self._make_dm()
        dm.setup("test")
        assert dm._test_dataset is not None
        assert len(dm._test_dataset) == 365

    def test_save_labels_mapping_43_classes(self, tmp_path: Path) -> None:
        """Phase 1 success criterion 5: labels_mapping.json has 43 classes."""
        dm = self._make_dm()
        out = tmp_path / "labels_mapping.json"
        dm.save_labels_mapping(out)
        assert out.exists()
        with open(out) as f:
            data = json.load(f)
        assert data["num_classes"] == 43
        assert len(data["class_to_idx"]) == 43
        assert len(data["idx_to_class"]) == 43
        assert data["class_to_idx"][""] == 0

    def test_train_dataloader_batch_shape(self) -> None:
        """Train batch must be (B, 3, 224, 224) float32 tensors."""
        dm = self._make_dm()
        dm.setup("fit")
        loader = dm.train_dataloader()
        images, labels = next(iter(loader))
        assert isinstance(images, torch.Tensor)
        assert images.dtype == torch.float32
        assert images.shape[1:] == (3, 224, 224)
        assert isinstance(labels, torch.Tensor)

    def test_val_dataloader_batch_shape(self) -> None:
        dm = self._make_dm()
        dm.setup("fit")
        loader = dm.val_dataloader()
        images, labels = next(iter(loader))
        assert images.shape[1:] == (3, 224, 224)
        assert images.dtype == torch.float32

    def test_class_weights_shape_and_positivity(self) -> None:
        dm = self._make_dm()
        dm.setup("fit")
        weights = dm.get_class_weights()
        assert weights.shape == (43,)
        assert (weights > 0).all()
```
  </action>
  <verify>
    ```bash
    cd "/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/code/classifier-training"
    pixi run test
    pixi run lint
    pixi run typecheck
    ```

    Expected outcome:
    - `test_dataset.py`: 6 tests pass
    - `test_datamodule.py` synthetic class: 10 tests pass
    - `test_datamodule.py` real dataset class: 8 tests pass (if dataset exists at expected path)
    - Total: 24+ tests pass (7 from plan 02 + 6 + 10 + 8 from plan 03)
    - `pixi run lint` exits 0
    - `pixi run typecheck` exits 0
  </verify>
  <done>
    - All synthetic tests (16 in test_dataset.py + test_datamodule.py synthetic class) pass
    - Real dataset tests pass if dataset present at `/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/data/basketball-jersey-numbers-ocr`
    - `pixi run lint` exits 0
    - `pixi run typecheck` exits 0
    - Phase 1 success criteria verified:
      1. `from classifier_training.data import ImageFolderDataModule` succeeds
      2. `pixi run pytest` passes
      3. `pixi run lint` and `pixi run typecheck` pass
      4. Val/test DataLoaders have no augmentation (determinism test passes)
      5. `labels_mapping.json` written with 43 alphabetically-ordered classes
  </done>
</task>

</tasks>

<verification>
Final Phase 1 verification — all five success criteria:

```bash
cd "/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/code/classifier-training"

# Criterion 1: importable
pixi run python -c "from classifier_training.data import ImageFolderDataModule; print('PASS criterion 1')"

# Criterion 2: tests pass
pixi run test

# Criterion 3: lint and typecheck
pixi run lint && pixi run typecheck && echo "PASS criterion 3"

# Criterion 4: no augmentation on val (covered by test_val_batch_is_deterministic)
# Criterion 5: labels_mapping.json with 43 classes (covered by test_save_labels_mapping_43_classes)
```

All five criteria satisfied when `pixi run test` reports all tests passing.
</verification>

<success_criteria>
- `from classifier_training.data import ImageFolderDataModule` succeeds
- All unit and integration tests pass (`pixi run test`)
- `pixi run lint` exits 0
- `pixi run typecheck` exits 0
- `test_val_batch_is_deterministic` passes — val transform has no augmentation
- Real dataset: `len(train_dataset) == 2930` (annotation rows, not image count)
- Real dataset: `num_classes == 43`, `class_to_idx[''] == 0`
- `labels_mapping.json` written with 43 entries in `class_to_idx`, `idx_to_class`, and normalization stats
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-and-data-pipeline/01-03-SUMMARY.md` following the summary template.
</output>
