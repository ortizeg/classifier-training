---
phase: 01-foundation-and-data-pipeline
plan: "02"
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/classifier_training/types.py
  - src/classifier_training/config.py
  - src/classifier_training/utils/__init__.py
  - tests/__init__.py
  - tests/conftest.py
  - tests/test_types_config.py
autonomous: true

must_haves:
  truths:
    - "`from classifier_training.types import ClassificationBatch` succeeds"
    - "`from classifier_training.config import DataModuleConfig` succeeds and validates fields"
    - "`DataModuleConfig(data_root='/tmp')` is frozen — attribute assignment raises `ValidationError`"
    - "`pixi run lint` still exits 0 after adding these files"
    - "`pixi run typecheck` still exits 0 after adding these files"
    - "`pixi run test` passes all tests in `test_types_config.py`"
  artifacts:
    - path: "src/classifier_training/types.py"
      provides: "ClassificationBatch TypedDict and type aliases used by DataModule and Model layers"
      exports: ["ClassificationBatch"]
    - path: "src/classifier_training/config.py"
      provides: "DataModuleConfig pydantic frozen model with all DataLoader parameters"
      exports: ["DataModuleConfig"]
    - path: "src/classifier_training/utils/__init__.py"
      provides: "Empty utils package init"
    - path: "tests/conftest.py"
      provides: "tmp_dataset_dir fixture — synthetic 3-class 3-split dataset with JSONL annotations"
    - path: "tests/test_types_config.py"
      provides: "Unit tests: DataModuleConfig validation, frozen enforcement, persistent_workers guard"
  key_links:
    - from: "src/classifier_training/config.py"
      to: "src/classifier_training/__init__.py"
      via: "same package — imports will flow through here in later plans"
      pattern: "from classifier_training.config import"
    - from: "tests/conftest.py"
      to: "tests/test_datamodule.py"
      via: "tmp_dataset_dir fixture shared across test modules"
      pattern: "def tmp_dataset_dir"
---

<objective>
Add the shared type contracts and Pydantic configuration model that the DataModule (Plan 03) will depend on. Also create the test infrastructure (conftest fixtures, test skeleton) that all Phase 1 tests share.

Purpose: Plan 03 imports `DataModuleConfig` and `ClassificationBatch` from this plan's output. The `tmp_dataset_dir` fixture in `conftest.py` is also needed by Plan 03's integration tests. This plan makes those contracts concrete and tested before the DataModule is built.
Output: `types.py`, `config.py`, `utils/__init__.py`, `tests/conftest.py`, `tests/test_types_config.py`.
</objective>

<execution_context>
@/Users/ortizeg/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ortizeg/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-and-data-pipeline/01-RESEARCH.md
@.planning/phases/01-foundation-and-data-pipeline/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: types.py, config.py, and utils package</name>
  <files>
    src/classifier_training/types.py
    src/classifier_training/config.py
    src/classifier_training/utils/__init__.py
  </files>
  <action>
Create `src/classifier_training/types.py`:

```python
"""Type aliases and TypedDicts for classifier_training inter-module contracts."""

from typing import TypedDict

import torch


class ClassificationBatch(TypedDict):
    """A single batch from a classification DataLoader.

    images: Float tensor of shape (B, C, H, W), normalized with ImageNet stats.
    labels: Long tensor of shape (B,), integer class indices.
    """

    images: torch.Tensor
    labels: torch.Tensor
```

Note: Keep `ClassificationBatch` as a TypedDict (not NamedTuple or dataclass) — this matches how Lightning receives batches via `training_step(self, batch, batch_idx)`. The Model layer (Phase 2) will unpack as `batch["images"], batch["labels"]`.

Create `src/classifier_training/config.py`:

```python
"""Pydantic frozen configuration models for classifier_training."""

from pydantic import BaseModel, model_validator


class DataModuleConfig(BaseModel, frozen=True):
    """Configuration for ImageFolderDataModule.

    All fields are validated at construction time. Frozen — no mutation after creation.
    """

    data_root: str
    batch_size: int = 32
    num_workers: int = 4
    pin_memory: bool = True
    persistent_workers: bool = True
    image_size: int = 224

    @model_validator(mode="after")
    def _persistent_workers_requires_workers(self) -> "DataModuleConfig":
        """persistent_workers=True with num_workers=0 silently does nothing in PyTorch."""
        if self.persistent_workers and self.num_workers == 0:
            # Use object.__setattr__ because model is frozen
            object.__setattr__(self, "persistent_workers", False)
        return self
```

Note: The `model_validator` auto-corrects `persistent_workers` when `num_workers=0`. This prevents a silent no-op that confuses DataLoader behaviour. The MPS num_workers guard lives in the DataModule itself (not in config) because it requires a runtime torch check.

Create `src/classifier_training/utils/__init__.py`:

```python
"""Utilities for classifier_training."""
```

This is intentionally minimal — FOUND-09 specifies `lightning.seed_everything` is used directly, not wrapped in a custom utility. No `seed.py` is needed.
  </action>
  <verify>
    ```bash
    cd "/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/code/classifier-training"
    pixi run python -c "
    from classifier_training.types import ClassificationBatch
    from classifier_training.config import DataModuleConfig
    cfg = DataModuleConfig(data_root='/tmp', num_workers=0)
    assert cfg.persistent_workers is False, 'validator must auto-correct persistent_workers'
    print('types and config OK')
    "
    pixi run lint
    pixi run typecheck
    ```
  </verify>
  <done>
    - `ClassificationBatch` and `DataModuleConfig` are importable
    - `DataModuleConfig(data_root='/tmp', num_workers=0).persistent_workers` is `False` (validator fires)
    - `DataModuleConfig(data_root='/tmp')` with defaults has `persistent_workers=True`, `num_workers=4`
    - `pixi run lint` exits 0
    - `pixi run typecheck` exits 0
  </done>
</task>

<task type="auto">
  <name>Task 2: Test infrastructure — conftest.py and test_types_config.py</name>
  <files>
    tests/__init__.py
    tests/conftest.py
    tests/test_types_config.py
  </files>
  <action>
Create `tests/__init__.py` (empty file — required for pytest discovery with src layout):
```python
```

Create `tests/conftest.py`. This fixture is the shared foundation for ALL Phase 1 tests including `test_datamodule.py` in Plan 03:

```python
"""Shared pytest fixtures for classifier_training tests."""

import json
from pathlib import Path

import pytest
from PIL import Image


@pytest.fixture()
def tmp_dataset_dir(tmp_path: Path) -> Path:
    """Minimal dataset in JSONL-annotated format for testing.

    Structure mirrors the real basketball-jersey-numbers-ocr dataset:
    - Flat image files per split (no class subdirectories)
    - annotations.jsonl with {"image": ..., "prefix": ..., "suffix": ...} per line

    3 splits x 3 classes x 2 images = 18 images total.
    Classes: "0", "1", "2" — 3 classes, all present in all splits.
    Train has 2 images per class (6 total) to allow sampler weight computation.
    """
    classes = ["0", "1", "2"]

    for split in ("train", "valid", "test"):
        split_dir = tmp_path / split
        split_dir.mkdir()
        lines: list[str] = []

        for cls in classes:
            for i in range(2):
                fname = f"img_{cls}_{i:02d}.jpg"
                img = Image.new("RGB", (224, 224), color=(int(cls) * 80, 100, 150))
                img.save(split_dir / fname)
                lines.append(
                    json.dumps({
                        "image": fname,
                        "prefix": "Read the number.",
                        "suffix": cls,
                    })
                )

        (split_dir / "annotations.jsonl").write_text("\n".join(lines) + "\n")

    return tmp_path
```

Create `tests/test_types_config.py`:

```python
"""Unit tests for classifier_training.types and classifier_training.config."""

import pytest
from pydantic import ValidationError

from classifier_training.config import DataModuleConfig
from classifier_training.types import ClassificationBatch


class TestDataModuleConfig:
    def test_defaults(self) -> None:
        cfg = DataModuleConfig(data_root="/tmp/data")
        assert cfg.data_root == "/tmp/data"
        assert cfg.batch_size == 32
        assert cfg.num_workers == 4
        assert cfg.pin_memory is True
        assert cfg.persistent_workers is True
        assert cfg.image_size == 224

    def test_frozen_raises_on_mutation(self) -> None:
        cfg = DataModuleConfig(data_root="/tmp/data")
        with pytest.raises(ValidationError):
            cfg.batch_size = 64  # type: ignore[misc]

    def test_persistent_workers_auto_corrected_when_num_workers_zero(self) -> None:
        cfg = DataModuleConfig(data_root="/tmp/data", num_workers=0, persistent_workers=True)
        assert cfg.persistent_workers is False, (
            "persistent_workers must be False when num_workers=0"
        )

    def test_persistent_workers_preserved_when_num_workers_nonzero(self) -> None:
        cfg = DataModuleConfig(data_root="/tmp/data", num_workers=4, persistent_workers=True)
        assert cfg.persistent_workers is True

    def test_persistent_workers_false_stays_false(self) -> None:
        cfg = DataModuleConfig(data_root="/tmp/data", num_workers=4, persistent_workers=False)
        assert cfg.persistent_workers is False

    def test_custom_values(self) -> None:
        cfg = DataModuleConfig(
            data_root="/data/jersey",
            batch_size=64,
            num_workers=8,
            pin_memory=False,
            image_size=256,
        )
        assert cfg.batch_size == 64
        assert cfg.num_workers == 8
        assert cfg.pin_memory is False
        assert cfg.image_size == 256


class TestClassificationBatchType:
    def test_typed_dict_keys(self) -> None:
        """ClassificationBatch has exactly 'images' and 'labels' keys."""
        import torch

        batch: ClassificationBatch = {
            "images": torch.zeros(4, 3, 224, 224),
            "labels": torch.zeros(4, dtype=torch.long),
        }
        assert "images" in batch
        assert "labels" in batch
        assert batch["images"].shape == (4, 3, 224, 224)
        assert batch["labels"].shape == (4,)
```
  </action>
  <verify>
    ```bash
    cd "/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/code/classifier-training"
    pixi run test
    ```
    Output must show all tests in `test_types_config.py` pass. Expected: 7 tests collected, 7 passed.
  </verify>
  <done>
    - `tests/__init__.py` exists (empty)
    - `tests/conftest.py` defines `tmp_dataset_dir` fixture producing 3-split/3-class synthetic dataset
    - `tests/test_types_config.py`: 7 tests, all pass
    - `pixi run test` exits 0
    - `pixi run lint` exits 0
    - `pixi run typecheck` exits 0
  </done>
</task>

</tasks>

<verification>
Final state check after both tasks:

```bash
cd "/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/code/classifier-training"
pixi run python -c "
from classifier_training.types import ClassificationBatch
from classifier_training.config import DataModuleConfig
cfg = DataModuleConfig(data_root='/tmp', num_workers=0)
assert cfg.persistent_workers is False
print('PASS: types and config verified')
"
pixi run lint
pixi run typecheck
pixi run test
```

All four commands must exit 0. `pixi run test` must report 7 tests passed.
</verification>

<success_criteria>
- `ClassificationBatch` and `DataModuleConfig` importable from `classifier_training`
- `DataModuleConfig` is frozen (mutation raises `ValidationError`)
- `DataModuleConfig(num_workers=0)` auto-corrects `persistent_workers` to `False`
- `tmp_dataset_dir` fixture exists in `conftest.py` producing proper JSONL-annotated synthetic dataset
- All 7 unit tests pass
- `pixi run lint`, `pixi run typecheck`, `pixi run test` all exit 0
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-and-data-pipeline/01-02-SUMMARY.md` following the summary template.
</output>
