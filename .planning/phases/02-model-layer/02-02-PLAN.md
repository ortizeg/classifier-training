---
phase: 02-model-layer
plan: "02"
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/classifier_training/models/__init__.py
  - src/classifier_training/models/resnet.py
  - src/classifier_training/conf/models/resnet18.yaml
  - src/classifier_training/conf/models/resnet50.yaml
  - tests/test_model.py
autonomous: true

must_haves:
  truths:
    - "pixi run python -c 'from classifier_training.models import ResNet18ClassificationModel, ResNet50ClassificationModel' exits 0"
    - "ResNet18ClassificationModel forward pass on (2, 3, 224, 224) returns logits shape (2, 43)"
    - "ResNet50ClassificationModel forward pass on (2, 3, 224, 224) returns logits shape (2, 43)"
    - "pixi run pytest tests/test_model.py passes — forward pass, loss, Pattern A metric update/compute/reset verified for both models"
    - "val/acc_top1, val/acc_top5, val/acc_class_N logged with no NaN or 0.0 artifacts in unit test"
    - "Hydra ConfigStore has 'resnet18' and 'resnet50' entries in group 'models' after import"
    - "resnet18.yaml and resnet50.yaml files exist with correct _target_ and lr=1e-4 / lr=5e-5 defaults"
  artifacts:
    - path: "src/classifier_training/models/resnet.py"
      provides: "ResNet18ClassificationModel and ResNet50ClassificationModel"
      exports: ["ResNet18ClassificationModel", "ResNet50ClassificationModel"]
      min_lines: 50
    - path: "src/classifier_training/models/__init__.py"
      provides: "Public model API"
      contains: "ResNet18ClassificationModel"
    - path: "src/classifier_training/conf/models/resnet18.yaml"
      provides: "Hydra config for ResNet18"
      contains: "_target_: classifier_training.models.resnet.ResNet18ClassificationModel"
    - path: "src/classifier_training/conf/models/resnet50.yaml"
      provides: "Hydra config for ResNet50"
      contains: "_target_: classifier_training.models.resnet.ResNet50ClassificationModel"
    - path: "tests/test_model.py"
      provides: "Model test suite"
      min_lines: 80
  key_links:
    - from: "src/classifier_training/models/resnet.py"
      to: "src/classifier_training/models/base.py"
      via: "class ResNet18ClassificationModel(BaseClassificationModel)"
      pattern: "class ResNet18ClassificationModel\\(BaseClassificationModel\\)"
    - from: "src/classifier_training/models/resnet.py"
      to: "src/classifier_training/utils/hydra.py"
      via: "@register decorator at class definition"
      pattern: "@register\\(name="
    - from: "src/classifier_training/conf/models/resnet18.yaml"
      to: "src/classifier_training/models/resnet.py"
      via: "_target_ field consumed by hydra.utils.instantiate"
      pattern: "_target_: classifier_training\\.models\\.resnet\\.ResNet18ClassificationModel"
    - from: "tests/test_model.py"
      to: "src/classifier_training/models/resnet.py"
      via: "import and forward pass test"
      pattern: "ResNet18ClassificationModel|ResNet50ClassificationModel"
---

<objective>
Implement ResNet18ClassificationModel and ResNet50ClassificationModel as concrete subclasses of BaseClassificationModel, register them with Hydra ConfigStore via @register, write the YAML configs for both, and add a test suite that verifies forward pass shape, loss computation, and Pattern A metric correctness.

Purpose: These are the primary model classes the training pipeline will use. Plan 02-02 closes all MODEL-01 through MODEL-09 requirements not covered by Plan 02-01.
Output: models/resnet.py (both ResNets), conf/models/*.yaml (Hydra configs), tests/test_model.py (full test suite), updated models/__init__.py.
</objective>

<execution_context>
@/Users/ortizeg/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ortizeg/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-model-layer/02-RESEARCH.md
@.planning/phases/02-model-layer/02-01-SUMMARY.md
@src/classifier_training/types.py
@src/classifier_training/models/base.py
@src/classifier_training/utils/hydra.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: ResNet18 and ResNet50 models, Hydra YAML configs, updated __init__.py</name>
  <files>
    src/classifier_training/models/resnet.py
    src/classifier_training/models/__init__.py
    src/classifier_training/conf/__init__.py
    src/classifier_training/conf/models/resnet18.yaml
    src/classifier_training/conf/models/resnet50.yaml
  </files>
  <action>
**1. Create src/classifier_training/models/resnet.py:**

Both models in a single file (they share >90% structure). Note the class names match the ROADMAP import check: `ResNet18ClassificationModel` and `ResNet50ClassificationModel` (not `ResNet18Model`).

```python
"""ResNet18 and ResNet50 classification models."""
from __future__ import annotations

import torch
import torchvision.models as tv_models

from classifier_training.models.base import BaseClassificationModel
from classifier_training.utils.hydra import register


@register(name="resnet18")
class ResNet18ClassificationModel(BaseClassificationModel):
    """ResNet18 backbone with ImageNet pretrained weights.

    fc replaced with Linear(512, num_classes).
    Pass pretrained=False in tests to skip the ~44MB weight download.
    """

    def __init__(
        self,
        num_classes: int = 43,
        pretrained: bool = True,
        **kwargs: object,
    ) -> None:
        super().__init__(num_classes=num_classes, **kwargs)
        weights = tv_models.ResNet18_Weights.DEFAULT if pretrained else None
        backbone = tv_models.resnet18(weights=weights)
        backbone.fc = torch.nn.Linear(backbone.fc.in_features, num_classes)
        self.model = backbone

    def forward(self, images: torch.Tensor) -> torch.Tensor:
        return self.model(images)  # type: ignore[no-any-return]


@register(name="resnet50")
class ResNet50ClassificationModel(BaseClassificationModel):
    """ResNet50 backbone with ImageNet pretrained weights.

    fc replaced with Linear(2048, num_classes).
    Pass pretrained=False in tests to skip the ~98MB weight download.
    """

    def __init__(
        self,
        num_classes: int = 43,
        pretrained: bool = True,
        **kwargs: object,
    ) -> None:
        super().__init__(num_classes=num_classes, **kwargs)
        weights = tv_models.ResNet50_Weights.DEFAULT if pretrained else None
        backbone = tv_models.resnet50(weights=weights)
        backbone.fc = torch.nn.Linear(backbone.fc.in_features, num_classes)
        self.model = backbone

    def forward(self, images: torch.Tensor) -> torch.Tensor:
        return self.model(images)  # type: ignore[no-any-return]
```

**Implementation notes:**
- `pretrained: bool = True` parameter lets tests pass `pretrained=False` to skip network downloads — avoids 44-98MB downloads on every test run
- `**kwargs: object` forwards all BaseClassificationModel hyperparams (learning_rate, weight_decay, etc.) — Hydra instantiation passes these from YAML
- `@register(name="resnet18")` stores into ConfigStore group="models" (inferred from `classifier_training.models.resnet` module path: `split(".")[-2]` = `"models"`)
- `# type: ignore[no-any-return]` on forward — torchvision returns `Any` from mypy's perspective
- Do NOT pass `class_weights` to `__init__` — it is a buffer set via `set_class_weights()` post-construction

**2. Update src/classifier_training/models/__init__.py** (the stub from Plan 02-01 only had BaseClassificationModel):

```python
"""Classification model implementations."""
from classifier_training.models.base import BaseClassificationModel
from classifier_training.models.resnet import (
    ResNet18ClassificationModel,
    ResNet50ClassificationModel,
)

__all__ = [
    "BaseClassificationModel",
    "ResNet18ClassificationModel",
    "ResNet50ClassificationModel",
]
```

**3. Create Hydra YAML config directory and files:**

Create `src/classifier_training/conf/` directory with `__init__.py` (empty — makes it a package so flit includes it):
```bash
mkdir -p src/classifier_training/conf/models
touch src/classifier_training/conf/__init__.py
touch src/classifier_training/conf/models/__init__.py
```

Create `src/classifier_training/conf/models/resnet18.yaml`:
```yaml
# Hydra config for ResNet18ClassificationModel
# Consumed by hydra.utils.instantiate(cfg.model) in the training script.
# Flat keys only — no nested wrapper (avoids double-nesting in ConfigStore).
_target_: classifier_training.models.resnet.ResNet18ClassificationModel
num_classes: 43
pretrained: true
learning_rate: 1.0e-4
weight_decay: 1.0e-4
warmup_epochs: 5
label_smoothing: 0.1
warmup_start_factor: 1.0e-3
cosine_eta_min_factor: 0.05
```

Create `src/classifier_training/conf/models/resnet50.yaml`:
```yaml
# Hydra config for ResNet50ClassificationModel
# ResNet50 is a larger model; slightly lower default LR appropriate for fine-tuning.
_target_: classifier_training.models.resnet.ResNet50ClassificationModel
num_classes: 43
pretrained: true
learning_rate: 5.0e-5
weight_decay: 1.0e-4
warmup_epochs: 5
label_smoothing: 0.1
warmup_start_factor: 1.0e-3
cosine_eta_min_factor: 0.05
```

**4. Verify Hydra ConfigStore registration:**
```bash
pixi run python -c "
import classifier_training.models  # triggers @register side-effects
from hydra.core.config_store import ConfigStore
cs = ConfigStore.instance()
# Access the registered nodes
print('ConfigStore groups:', list(cs.repo.list('models')))
"
```
Expected output includes `resnet18` and `resnet50` entries.

**5. Run lint and typecheck:**
```bash
pixi run lint
pixi run typecheck
```
  </action>
  <verify>
```bash
# Import check (matches Phase 2 success criterion 2)
pixi run python -c "
from classifier_training.models import ResNet18ClassificationModel, ResNet50ClassificationModel
import torch

# Forward pass shape check — no pretrained download
r18 = ResNet18ClassificationModel(num_classes=43, pretrained=False)
r50 = ResNet50ClassificationModel(num_classes=43, pretrained=False)
x = torch.randn(2, 3, 224, 224)
assert r18(x).shape == (2, 43), f'ResNet18 shape wrong: {r18(x).shape}'
assert r50(x).shape == (2, 43), f'ResNet50 shape wrong: {r50(x).shape}'
print('Forward pass shapes: OK')
"
pixi run lint
pixi run typecheck
```
All commands exit 0. Forward pass shapes confirmed (2, 43).
  </verify>
  <done>
ResNet18ClassificationModel and ResNet50ClassificationModel importable, forward pass returns (2, 43) shape, Hydra ConfigStore has resnet18 and resnet50 entries in group "models", resnet18.yaml and resnet50.yaml exist, lint and typecheck pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Test suite for both ResNet models — forward pass, loss, Pattern A metrics, optimizer</name>
  <files>
    tests/test_model.py
  </files>
  <action>
Create `tests/test_model.py`. All tests use `pretrained=False` to avoid weight downloads. Use `num_classes=3` for small fixture tests (exercises the `min(5, num_classes)` guard), and `num_classes=43` for full-dataset checks.

```python
"""Tests for BaseClassificationModel, ResNet18ClassificationModel, ResNet50ClassificationModel."""
from __future__ import annotations

import pytest
import torch
import lightning as L

from classifier_training.models import (
    BaseClassificationModel,
    ResNet18ClassificationModel,
    ResNet50ClassificationModel,
)
from classifier_training.types import ClassificationBatch


# ---------------------------------------------------------------------------
# Fixtures
# ---------------------------------------------------------------------------

@pytest.fixture()
def batch_3cls() -> ClassificationBatch:
    """Small 3-class batch: B=4, 3-channel 64x64 images."""
    return {
        "images": torch.randn(4, 3, 64, 64),
        "labels": torch.randint(0, 3, (4,)),
    }


@pytest.fixture()
def batch_43cls() -> ClassificationBatch:
    """Full 43-class batch: B=4, 3-channel 224x224 images."""
    return {
        "images": torch.randn(4, 3, 224, 224),
        "labels": torch.randint(0, 43, (4,)),
    }


@pytest.fixture()
def resnet18_3cls() -> ResNet18ClassificationModel:
    return ResNet18ClassificationModel(num_classes=3, pretrained=False)


@pytest.fixture()
def resnet50_3cls() -> ResNet50ClassificationModel:
    return ResNet50ClassificationModel(num_classes=3, pretrained=False)


@pytest.fixture()
def resnet18_43cls() -> ResNet18ClassificationModel:
    return ResNet18ClassificationModel(num_classes=43, pretrained=False)


@pytest.fixture()
def resnet50_43cls() -> ResNet50ClassificationModel:
    return ResNet50ClassificationModel(num_classes=43, pretrained=False)


# ---------------------------------------------------------------------------
# BaseClassificationModel: metric guard
# ---------------------------------------------------------------------------

class TestBaseMetricGuard:
    def test_top5_guard_3_classes(self) -> None:
        """top_k_5 = min(5, 3) = 3 — no ValueError from MulticlassAccuracy."""
        m = ResNet18ClassificationModel(num_classes=3, pretrained=False)
        assert m.val_top5.top_k == 3

    def test_top5_guard_43_classes(self) -> None:
        """top_k_5 = min(5, 43) = 5 — full top-5 for real dataset."""
        m = ResNet18ClassificationModel(num_classes=43, pretrained=False)
        assert m.val_top5.top_k == 5

    def test_class_weights_buffer_shape(self, resnet18_3cls: ResNet18ClassificationModel) -> None:
        """class_weights buffer has shape (num_classes,) and defaults to ones."""
        assert resnet18_3cls.class_weights.shape == (3,)
        assert torch.allclose(resnet18_3cls.class_weights, torch.ones(3))

    def test_set_class_weights_updates_buffer(self, resnet18_3cls: ResNet18ClassificationModel) -> None:
        """set_class_weights copies new tensor into buffer and rebuilds loss_fn."""
        new_weights = torch.tensor([1.0, 2.0, 3.0])
        resnet18_3cls.set_class_weights(new_weights)
        assert torch.allclose(resnet18_3cls.class_weights, new_weights)

    def test_hparams_saved(self, resnet18_3cls: ResNet18ClassificationModel) -> None:
        """save_hyperparameters stores scalar params; class_weights is NOT in hparams."""
        assert resnet18_3cls.hparams["num_classes"] == 3
        assert resnet18_3cls.hparams["learning_rate"] == pytest.approx(1e-4)
        assert "class_weights" not in resnet18_3cls.hparams


# ---------------------------------------------------------------------------
# ResNet18: forward pass and loss
# ---------------------------------------------------------------------------

class TestResNet18ForwardPass:
    def test_forward_shape_3cls(
        self,
        resnet18_3cls: ResNet18ClassificationModel,
        batch_3cls: ClassificationBatch,
    ) -> None:
        logits = resnet18_3cls(batch_3cls["images"])
        assert logits.shape == (4, 3)

    def test_forward_shape_43cls(
        self,
        resnet18_43cls: ResNet18ClassificationModel,
        batch_43cls: ClassificationBatch,
    ) -> None:
        logits = resnet18_43cls(batch_43cls["images"])
        assert logits.shape == (4, 43)

    def test_loss_is_finite(
        self,
        resnet18_3cls: ResNet18ClassificationModel,
        batch_3cls: ClassificationBatch,
    ) -> None:
        logits = resnet18_3cls(batch_3cls["images"])
        loss = resnet18_3cls.loss_fn(logits, batch_3cls["labels"])
        assert torch.isfinite(loss).all()

    def test_training_step_returns_tensor(
        self,
        resnet18_3cls: ResNet18ClassificationModel,
        batch_3cls: ClassificationBatch,
    ) -> None:
        # training_step needs a trainer context for self.log — use Trainer
        trainer = L.Trainer(
            max_epochs=1, logger=False, enable_checkpointing=False,
            enable_progress_bar=False, enable_model_summary=False,
        )
        # Attach trainer to model for self.log to work
        resnet18_3cls.trainer = trainer  # type: ignore[assignment]
        loss = resnet18_3cls.training_step(batch_3cls, 0)
        assert isinstance(loss, torch.Tensor)
        assert loss.ndim == 0  # scalar


# ---------------------------------------------------------------------------
# ResNet50: forward pass and loss
# ---------------------------------------------------------------------------

class TestResNet50ForwardPass:
    def test_forward_shape_3cls(
        self,
        resnet50_3cls: ResNet50ClassificationModel,
        batch_3cls: ClassificationBatch,
    ) -> None:
        logits = resnet50_3cls(batch_3cls["images"])
        assert logits.shape == (4, 3)

    def test_forward_shape_43cls(
        self,
        resnet50_43cls: ResNet50ClassificationModel,
        batch_43cls: ClassificationBatch,
    ) -> None:
        logits = resnet50_43cls(batch_43cls["images"])
        assert logits.shape == (4, 43)

    def test_loss_is_finite(
        self,
        resnet50_3cls: ResNet50ClassificationModel,
        batch_3cls: ClassificationBatch,
    ) -> None:
        logits = resnet50_3cls(batch_3cls["images"])
        loss = resnet50_3cls.loss_fn(logits, batch_3cls["labels"])
        assert torch.isfinite(loss).all()


# ---------------------------------------------------------------------------
# Pattern A metrics: update/compute/reset discipline
# ---------------------------------------------------------------------------

class TestPatternAMetrics:
    """Verify Pattern A: update in step, compute+log+reset in epoch_end.
    Direct unit test — no Trainer needed for update/compute/reset cycle.
    """

    def test_val_top1_no_nan_after_update(
        self,
        resnet18_3cls: ResNet18ClassificationModel,
        batch_3cls: ClassificationBatch,
    ) -> None:
        """After update(), compute() returns finite value (not NaN or 0.0 from empty state)."""
        logits = resnet18_3cls(batch_3cls["images"])
        resnet18_3cls.val_top1.update(logits, batch_3cls["labels"])
        result = resnet18_3cls.val_top1.compute()
        assert torch.isfinite(result), f"val_top1 NaN: {result}"
        resnet18_3cls.val_top1.reset()

    def test_val_top5_no_nan_after_update(
        self,
        resnet18_3cls: ResNet18ClassificationModel,
        batch_3cls: ClassificationBatch,
    ) -> None:
        """val_top5 uses top_k=min(5,3)=3 — update/compute/reset without error."""
        logits = resnet18_3cls(batch_3cls["images"])
        resnet18_3cls.val_top5.update(logits, batch_3cls["labels"])
        result = resnet18_3cls.val_top5.compute()
        assert torch.isfinite(result), f"val_top5 NaN: {result}"
        resnet18_3cls.val_top5.reset()

    def test_val_per_cls_shape_and_finite(
        self,
        resnet18_3cls: ResNet18ClassificationModel,
        batch_3cls: ClassificationBatch,
    ) -> None:
        """val_per_cls returns shape (num_classes,) with all finite values."""
        logits = resnet18_3cls(batch_3cls["images"])
        resnet18_3cls.val_per_cls.update(logits, batch_3cls["labels"])
        result = resnet18_3cls.val_per_cls.compute()
        assert result.shape == (3,), f"per_cls shape: {result.shape}"
        assert torch.isfinite(result).all(), f"per_cls NaN: {result}"
        resnet18_3cls.val_per_cls.reset()

    def test_reset_clears_state(
        self,
        resnet18_3cls: ResNet18ClassificationModel,
        batch_3cls: ClassificationBatch,
    ) -> None:
        """After reset(), a second update+compute cycle gives consistent result (not accumulated)."""
        logits = resnet18_3cls(batch_3cls["images"])
        # First cycle
        resnet18_3cls.val_top1.update(logits, batch_3cls["labels"])
        result1 = resnet18_3cls.val_top1.compute()
        resnet18_3cls.val_top1.reset()
        # Second cycle — same data, should match
        resnet18_3cls.val_top1.update(logits, batch_3cls["labels"])
        result2 = resnet18_3cls.val_top1.compute()
        resnet18_3cls.val_top1.reset()
        assert torch.allclose(result1, result2), (
            f"reset() broken: {result1} != {result2}"
        )

    def test_train_top1_update_compute_reset(
        self,
        resnet18_3cls: ResNet18ClassificationModel,
        batch_3cls: ClassificationBatch,
    ) -> None:
        logits = resnet18_3cls(batch_3cls["images"])
        resnet18_3cls.train_top1.update(logits, batch_3cls["labels"])
        result = resnet18_3cls.train_top1.compute()
        assert torch.isfinite(result)
        resnet18_3cls.train_top1.reset()


# ---------------------------------------------------------------------------
# Optimizer and scheduler structure
# ---------------------------------------------------------------------------

class TestOptimizerScheduler:
    def test_configure_optimizers_structure(
        self,
        resnet18_3cls: ResNet18ClassificationModel,
    ) -> None:
        """configure_optimizers returns dict with optimizer and lr_scheduler keys."""
        # Attach a mock trainer so self.trainer.max_epochs is accessible
        trainer = L.Trainer(
            max_epochs=20, logger=False, enable_checkpointing=False,
            enable_progress_bar=False, enable_model_summary=False,
        )
        resnet18_3cls.trainer = trainer  # type: ignore[assignment]
        result = resnet18_3cls.configure_optimizers()
        assert isinstance(result, dict)
        assert "optimizer" in result
        assert "lr_scheduler" in result
        assert isinstance(result["optimizer"], torch.optim.AdamW)
        assert result["lr_scheduler"]["interval"] == "epoch"

    def test_optimizer_lr(self, resnet18_43cls: ResNet18ClassificationModel) -> None:
        """AdamW param group lr matches default learning_rate=1e-4."""
        trainer = L.Trainer(
            max_epochs=20, logger=False, enable_checkpointing=False,
            enable_progress_bar=False, enable_model_summary=False,
        )
        resnet18_43cls.trainer = trainer  # type: ignore[assignment]
        result = resnet18_43cls.configure_optimizers()
        lr = result["optimizer"].param_groups[0]["lr"]
        assert lr == pytest.approx(1e-4)


# ---------------------------------------------------------------------------
# Hydra ConfigStore registration
# ---------------------------------------------------------------------------

class TestHydraRegistration:
    def test_resnet18_registered(self) -> None:
        """@register stores ResNet18ClassificationModel in ConfigStore group='models'."""
        import classifier_training.models  # noqa: F401 — triggers @register side-effects
        from hydra.core.config_store import ConfigStore
        cs = ConfigStore.instance()
        # ConfigStore.list() returns node names under the group path
        nodes = cs.repo.list("models")
        names = [n.name for n in nodes]
        assert "resnet18" in names, f"resnet18 not in ConfigStore models: {names}"

    def test_resnet50_registered(self) -> None:
        import classifier_training.models  # noqa: F401
        from hydra.core.config_store import ConfigStore
        cs = ConfigStore.instance()
        nodes = cs.repo.list("models")
        names = [n.name for n in nodes]
        assert "resnet50" in names, f"resnet50 not in ConfigStore models: {names}"
```

**After writing test file:**
```bash
pixi run lint tests/test_model.py
pixi run pytest tests/test_model.py -v
```

**Expected failures to fix before committing:**
- If `self.trainer = trainer` assignment fails mypy: add `# type: ignore[assignment]` (already included above)
- If `training_step` fails because `self.log` requires trainer: the test fixture attaches trainer correctly
- ConfigStore `cs.repo.list("models")` API: if this differs in installed hydra version, use `cs.list("")` or check ConfigStore docs for the installed version. Fall back to: `from hydra._internal.config_repository import ConfigRepository` if needed.
  </action>
  <verify>
```bash
# Run model test suite (Phase 2 success criterion 1)
pixi run pytest tests/test_model.py -v

# Run full test suite — existing 34 tests must still pass
pixi run pytest -v

# Confirm import statement from Phase 2 success criterion 2
pixi run python -c "from classifier_training.models import ResNet18ClassificationModel, ResNet50ClassificationModel; print('import OK')"

# Lint and typecheck still pass
pixi run lint
pixi run typecheck
```
All commands exit 0. `tests/test_model.py` shows all tests pass with no NaN assertions failing.
  </verify>
  <done>
pixi run pytest tests/test_model.py passes for all tests, full test suite (existing 34 + new model tests) passes, ResNet18ClassificationModel and ResNet50ClassificationModel importable from classifier_training.models, ConfigStore has resnet18 and resnet50 entries, lint and typecheck pass.
  </done>
</task>

</tasks>

<verification>
End-to-end Phase 2 success criteria verification:

```bash
# Criterion 1: test_model.py passes
pixi run pytest tests/test_model.py -v

# Criterion 2: both models importable
pixi run python -c "
from classifier_training.models import ResNet18ClassificationModel, ResNet50ClassificationModel
print('Criterion 2: PASS')
"

# Criterion 3: Pattern A correctness (covered by TestPatternAMetrics)
# Confirmed if test_model.py passes — tests explicitly check no NaN/0.0

# Criterion 4: Hydra config override selects correct backbone
pixi run python -c "
from omegaconf import OmegaConf
import yaml, pathlib
r18 = yaml.safe_load(pathlib.Path('src/classifier_training/conf/models/resnet18.yaml').read_text())
r50 = yaml.safe_load(pathlib.Path('src/classifier_training/conf/models/resnet50.yaml').read_text())
assert r18['_target_'].endswith('ResNet18ClassificationModel'), r18
assert r50['_target_'].endswith('ResNet50ClassificationModel'), r50
assert r18['learning_rate'] == 1e-4, r18['learning_rate']
assert r50['learning_rate'] == 5e-5, r50['learning_rate']
print('Criterion 4: PASS — YAML configs correct')
"

# Full suite
pixi run pytest -v
pixi run lint
pixi run typecheck
```
</verification>

<success_criteria>
1. `pixi run pytest tests/test_model.py` exits 0 — all model tests pass
2. `from classifier_training.models import ResNet18ClassificationModel, ResNet50ClassificationModel` succeeds
3. `ResNet18ClassificationModel(num_classes=3, pretrained=False)(torch.randn(4,3,64,64)).shape == (4, 3)` — forward pass correct
4. `ResNet50ClassificationModel(num_classes=43, pretrained=False)(torch.randn(2,3,224,224)).shape == (2, 43)` — forward pass correct
5. Pattern A tests pass — val_top1, val_top5, val_per_cls all finite after update/compute/reset cycle
6. ConfigStore has "resnet18" and "resnet50" in group "models"
7. resnet18.yaml `_target_` is `classifier_training.models.resnet.ResNet18ClassificationModel` with `learning_rate: 1.0e-4`
8. resnet50.yaml `_target_` is `classifier_training.models.resnet.ResNet50ClassificationModel` with `learning_rate: 5.0e-5`
9. `pixi run pytest` passes full suite (existing 34 + new model tests)
10. `pixi run lint` and `pixi run typecheck` exit 0
</success_criteria>

<output>
After completion, create `.planning/phases/02-model-layer/02-02-SUMMARY.md` using the summary template.
</output>
