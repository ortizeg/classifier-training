---
phase: 02-model-layer
plan: "01"
type: execute
wave: 1
depends_on: []
files_modified:
  - pixi.toml
  - pyproject.toml
  - src/classifier_training/utils/__init__.py
  - src/classifier_training/utils/hydra.py
  - src/classifier_training/models/__init__.py
  - src/classifier_training/models/base.py
autonomous: true

must_haves:
  truths:
    - "pixi run python -c 'import hydra' exits 0 — hydra-core is installed in the pixi env"
    - "BaseClassificationModel can be instantiated with num_classes=3 without error"
    - "A forward pass through BaseClassificationModel with a (2, 3, 224, 224) tensor returns logits of shape (2, 3)"
    - "training_step and validation_step execute without NaN or 0.0 in metric state"
    - "on_validation_epoch_end logs val/acc_top1, val/acc_top5, and val/acc_class_N for all classes"
    - "configure_optimizers returns AdamW optimizer + SequentialLR(LinearLR + CosineAnnealingLR)"
    - "pixi run lint and pixi run typecheck both pass after adding hydra.* and torchmetrics.* to mypy overrides"
  artifacts:
    - path: "pixi.toml"
      provides: "hydra-core dependency declaration"
      contains: "hydra-core"
    - path: "pyproject.toml"
      provides: "hydra-core, omegaconf, torchmetrics in project deps + mypy overrides"
      contains: "hydra-core"
    - path: "src/classifier_training/utils/hydra.py"
      provides: "@register decorator for Hydra ConfigStore"
      exports: ["register"]
    - path: "src/classifier_training/models/base.py"
      provides: "BaseClassificationModel LightningModule"
      exports: ["BaseClassificationModel"]
      min_lines: 80
    - path: "src/classifier_training/models/__init__.py"
      provides: "public package init (concrete exports added in plan 02-02)"
      contains: "BaseClassificationModel"
  key_links:
    - from: "src/classifier_training/models/base.py"
      to: "src/classifier_training/utils/hydra.py"
      via: "import register"
      pattern: "from classifier_training\\.utils\\.hydra import register"
    - from: "src/classifier_training/models/base.py"
      to: "torchmetrics.classification.MulticlassAccuracy"
      via: "metric instantiation in __init__"
      pattern: "MulticlassAccuracy"
    - from: "src/classifier_training/models/base.py"
      to: "torch.optim.lr_scheduler.SequentialLR"
      via: "configure_optimizers"
      pattern: "SequentialLR"
---

<objective>
Add hydra-core to the pixi environment, copy the @register decorator from the sibling repo, and implement BaseClassificationModel — the LightningModule ABC with CrossEntropyLoss (class-weight buffer), Pattern A torchmetrics logging (Top-1/Top-5/per-class MulticlassAccuracy), and AdamW + SequentialLR(LinearLR warmup + CosineAnnealingLR) optimizer.

Purpose: All concrete ResNet models (Plan 02-02) subclass BaseClassificationModel. The base must be correct and lint/typecheck-clean before any subclass is written.
Output: pixi.toml with hydra-core, pyproject.toml updated, utils/hydra.py @register, models/base.py BaseClassificationModel, models/__init__.py stub.
</objective>

<execution_context>
@/Users/ortizeg/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ortizeg/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-model-layer/02-RESEARCH.md
@src/classifier_training/types.py
@src/classifier_training/data/datamodule.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add hydra-core to pixi.toml and pyproject.toml, create utils/hydra.py @register decorator</name>
  <files>
    pixi.toml
    pyproject.toml
    src/classifier_training/utils/__init__.py
    src/classifier_training/utils/hydra.py
  </files>
  <action>
**1. Update pixi.toml — add hydra-core to [dependencies]:**

Add `hydra-core = "*"` alongside the existing pytorch, torchvision, lightning entries. Also add `torchmetrics = "*"` explicitly (currently in env from conda-forge but not declared). Do not change any existing entries.

**2. Update pyproject.toml — three changes:**

a) Under `[project] dependencies`, add:
```
"hydra-core",
"omegaconf",
"torchmetrics",
```
Place after the existing `"lightning"` entry.

b) Under `[[tool.mypy.overrides]] module` list, append:
```
"hydra.*",
"omegaconf.*",
"torchmetrics.*",
```

**3. Run pixi install to pull hydra-core into the environment:**
```bash
pixi install
```
Verify with:
```bash
pixi run python -c "import hydra; print(hydra.__version__)"
```

**4. Create src/classifier_training/utils/hydra.py — copy @register decorator verbatim from sibling repo:**

The source is at `/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/code/object-detection-training/src/object_detection_training/utils/hydra.py`. Copy it without modification — it is already correct for this project's structure. The decorator infers Hydra ConfigStore `group` from `target_cls.__module__.split(".")[-2]`; for `classifier_training.models.resnet.ResNet18Model` this yields group `"models"`, which matches where YAML configs will be stored.

**5. Update src/classifier_training/utils/__init__.py:**

Re-export `register` so callers can do `from classifier_training.utils import register` if needed:
```python
"""Utility helpers for classifier_training."""
from classifier_training.utils.hydra import register

__all__ = ["register"]
```

**6. Run lint and typecheck after creating utils/hydra.py:**
```bash
pixi run lint
pixi run typecheck
```
Fix any issues before committing. Common issue: hydra.* and omegaconf.* must be in mypy overrides (step 2b) or mypy will report missing stubs.
  </action>
  <verify>
```bash
pixi run python -c "import hydra; from classifier_training.utils.hydra import register; print('OK')"
pixi run lint
pixi run typecheck
```
All three commands exit 0.
  </verify>
  <done>
hydra-core importable in pixi env, @register decorator importable from classifier_training.utils.hydra, lint and typecheck pass with no new errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement BaseClassificationModel with Pattern A metrics, CrossEntropyLoss buffer, and AdamW + SequentialLR</name>
  <files>
    src/classifier_training/models/__init__.py
    src/classifier_training/models/base.py
  </files>
  <action>
**Create src/classifier_training/models/__init__.py** (stub — concrete model exports added in Plan 02-02):
```python
"""Classification model implementations."""
from classifier_training.models.base import BaseClassificationModel

__all__ = ["BaseClassificationModel"]
```

**Create src/classifier_training/models/base.py** implementing BaseClassificationModel exactly as verified in the research:

```python
"""Base LightningModule for all classification models."""
from __future__ import annotations

import lightning as L
import torch
from loguru import logger
from torchmetrics.classification import MulticlassAccuracy

from classifier_training.types import ClassificationBatch


class BaseClassificationModel(L.LightningModule):
    """Abstract base for ResNet classification models.

    Subclasses must set self.model (nn.Module backbone) in __init__ and
    implement forward(). Do NOT pass class_weights to __init__; call
    set_class_weights() from the training script after datamodule.setup().
    """

    def __init__(
        self,
        num_classes: int,
        learning_rate: float = 1e-4,
        weight_decay: float = 1e-4,
        warmup_epochs: int = 5,
        label_smoothing: float = 0.1,
        warmup_start_factor: float = 1e-3,
        cosine_eta_min_factor: float = 0.05,
    ) -> None:
        super().__init__()
        self.save_hyperparameters()

        # Register class_weights as buffer so .to(device) transfers it
        # automatically. Default ones; override via set_class_weights().
        self.register_buffer("class_weights", torch.ones(num_classes))
        self._build_loss_fn()

        # Pattern A metrics — one set per split, auto device placement.
        # top_k_5 guard: MulticlassAccuracy raises ValueError if top_k > num_classes.
        top_k_5 = min(5, num_classes)
        self.train_top1 = MulticlassAccuracy(
            num_classes=num_classes, top_k=1, average="micro"
        )
        self.val_top1 = MulticlassAccuracy(
            num_classes=num_classes, top_k=1, average="micro"
        )
        self.val_top5 = MulticlassAccuracy(
            num_classes=num_classes, top_k=top_k_5, average="micro"
        )
        self.val_per_cls = MulticlassAccuracy(
            num_classes=num_classes, top_k=1, average="none"
        )
        self.test_top1 = MulticlassAccuracy(
            num_classes=num_classes, top_k=1, average="micro"
        )
        self.test_top5 = MulticlassAccuracy(
            num_classes=num_classes, top_k=top_k_5, average="micro"
        )
        self.test_per_cls = MulticlassAccuracy(
            num_classes=num_classes, top_k=1, average="none"
        )

    def _build_loss_fn(self) -> None:
        """Build CrossEntropyLoss from registered class_weights buffer."""
        self.loss_fn = torch.nn.CrossEntropyLoss(
            weight=self.class_weights,
            label_smoothing=self.hparams["label_smoothing"],
        )

    def set_class_weights(self, weights: torch.Tensor) -> None:
        """Update class weights buffer and rebuild loss function.

        Call this from the training script after datamodule.setup():
            model.set_class_weights(dm.get_class_weights())
        """
        self.class_weights.copy_(weights.to(self.class_weights.device))
        self._build_loss_fn()

    def training_step(
        self, batch: ClassificationBatch, batch_idx: int
    ) -> torch.Tensor:
        images, labels = batch["images"], batch["labels"]
        logits = self(images)
        loss = self.loss_fn(logits, labels)
        self.log(
            "train/loss", loss, on_step=True, on_epoch=True, prog_bar=True
        )
        # Pattern A: update only in step; compute+log+reset in epoch_end
        self.train_top1.update(logits, labels)
        return loss

    def on_train_epoch_end(self) -> None:
        self.log("train/acc_top1", self.train_top1.compute())
        self.train_top1.reset()

    def validation_step(
        self, batch: ClassificationBatch, batch_idx: int
    ) -> None:
        images, labels = batch["images"], batch["labels"]
        logits = self(images)
        loss = self.loss_fn(logits, labels)
        self.log(
            "val/loss", loss, on_step=False, on_epoch=True, prog_bar=True
        )
        self.val_top1.update(logits, labels)
        self.val_top5.update(logits, labels)
        self.val_per_cls.update(logits, labels)

    def on_validation_epoch_end(self) -> None:
        self.log("val/acc_top1", self.val_top1.compute(), prog_bar=True)
        self.log("val/acc_top5", self.val_top5.compute())
        per_cls = self.val_per_cls.compute()
        for i, acc in enumerate(per_cls):
            self.log(f"val/acc_class_{i}", acc)
        self.val_top1.reset()
        self.val_top5.reset()
        self.val_per_cls.reset()

    def test_step(
        self, batch: ClassificationBatch, batch_idx: int
    ) -> None:
        images, labels = batch["images"], batch["labels"]
        logits = self(images)
        loss = self.loss_fn(logits, labels)
        self.log("test/loss", loss, on_step=False, on_epoch=True)
        self.test_top1.update(logits, labels)
        self.test_top5.update(logits, labels)
        self.test_per_cls.update(logits, labels)

    def on_test_epoch_end(self) -> None:
        self.log("test/acc_top1", self.test_top1.compute())
        self.log("test/acc_top5", self.test_top5.compute())
        per_cls = self.test_per_cls.compute()
        for i, acc in enumerate(per_cls):
            self.log(f"test/acc_class_{i}", acc)
        self.test_top1.reset()
        self.test_top5.reset()
        self.test_per_cls.reset()

    def configure_optimizers(self) -> dict:  # type: ignore[override]
        from torch.optim.lr_scheduler import (
            CosineAnnealingLR,
            LinearLR,
            SequentialLR,
        )

        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=self.hparams["learning_rate"],
            weight_decay=self.hparams["weight_decay"],
        )
        max_epochs = (
            (self.trainer.max_epochs or 100) if self.trainer else 100
        )
        warmup = self.hparams["warmup_epochs"]
        cosine_epochs = max(1, max_epochs - warmup)
        eta_min = (
            self.hparams["learning_rate"]
            * self.hparams["cosine_eta_min_factor"]
        )

        warmup_sched = LinearLR(
            optimizer,
            start_factor=self.hparams["warmup_start_factor"],
            end_factor=1.0,
            total_iters=max(1, warmup),
        )
        cosine_sched = CosineAnnealingLR(
            optimizer,
            T_max=cosine_epochs,
            eta_min=eta_min,
        )
        scheduler = SequentialLR(
            optimizer,
            schedulers=[warmup_sched, cosine_sched],
            milestones=[warmup],
        )
        return {
            "optimizer": optimizer,
            "lr_scheduler": {"scheduler": scheduler, "interval": "epoch"},
        }
```

**Key implementation notes:**
- Use `self.hparams["key"]` (dict access) not `self.hparams.key` (attribute access) — mypy strict mode can reject attribute access on the hparams Namespace depending on the lightning version's type stubs
- `class_weights` is a `register_buffer`, NOT a constructor argument — prevents `save_hyperparameters()` serialization failure (tensors are not JSON-serializable)
- `top_k_5 = min(5, num_classes)` is critical — test fixtures use 3-class datasets; hardcoding `top_k=5` raises ValueError
- Do NOT use Pattern B: never `self.log("val/acc_top1", self.val_top1)` — passing metric object to log() causes NaN/0.0 on first epoch
- `# type: ignore[override]` on `configure_optimizers` if mypy flags the return type mismatch with LightningModule's declared signature

**After writing files, run:**
```bash
pixi run lint
pixi run typecheck
```
Fix any issues. Common mypy issues:
- `self.hparams` type may need `# type: ignore[index]` if hparams typed as non-subscriptable
- `per_cls` iteration may need explicit `Tensor` type annotation
  </action>
  <verify>
```bash
pixi run python -c "
from classifier_training.models import BaseClassificationModel
import torch
# Instantiate with 3 classes (simulates test fixtures)
m = BaseClassificationModel(num_classes=3)
x = torch.randn(2, 3, 224, 224)
# BaseClassificationModel has no forward — test subclass wiring in 02-02
print('BaseClassificationModel instantiation: OK')
print('Hparams:', m.hparams)
print('class_weights shape:', m.class_weights.shape)
"
pixi run lint
pixi run typecheck
```
All three commands exit 0. The instantiation print confirms `num_classes=3`, `learning_rate=0.0001`, and `class_weights` shape `(3,)`.
  </verify>
  <done>
BaseClassificationModel importable from classifier_training.models, instantiates without error with num_classes=3, hparams saved correctly, class_weights buffer shape matches num_classes, lint and typecheck pass.
  </done>
</task>

</tasks>

<verification>
End-to-end verification after both tasks complete:

```bash
# 1. hydra-core installed
pixi run python -c "import hydra; print('hydra', hydra.__version__)"

# 2. @register importable
pixi run python -c "from classifier_training.utils.hydra import register; print('register OK')"

# 3. BaseClassificationModel importable and instantiable
pixi run python -c "
from classifier_training.models import BaseClassificationModel
import torch
m = BaseClassificationModel(num_classes=43)
print('hparams:', dict(m.hparams))
print('class_weights:', m.class_weights.shape)
print('train_top1:', m.train_top1)
print('val_top5 top_k:', m.val_top5.top_k)  # should be 5
m3 = BaseClassificationModel(num_classes=3)
print('val_top5 top_k (3-class):', m3.val_top5.top_k)  # should be 3
"

# 4. Lint and typecheck pass
pixi run lint
pixi run typecheck

# 5. Existing 34 tests still pass
pixi run pytest tests/ -x -q
```
</verification>

<success_criteria>
1. `import hydra` succeeds in pixi shell (hydra-core installed)
2. `from classifier_training.utils.hydra import register` succeeds
3. `from classifier_training.models import BaseClassificationModel` succeeds
4. `BaseClassificationModel(num_classes=3).val_top5.top_k == 3` (min(5,3) guard works)
5. `BaseClassificationModel(num_classes=43).val_top5.top_k == 5` (full dataset)
6. `pixi run lint` exits 0
7. `pixi run typecheck` exits 0
8. `pixi run pytest tests/ -x -q` still passes all 34 existing tests
</success_criteria>

<output>
After completion, create `.planning/phases/02-model-layer/02-01-SUMMARY.md` using the summary template.
</output>
