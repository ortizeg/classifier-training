---
phase: 03-callbacks-and-onnx-export
plan: 03
type: execute
wave: 3
depends_on: ["03-01", "03-02"]
files_modified:
  - src/classifier_training/conf/callbacks/default.yaml
  - tests/test_callbacks_integration.py
autonomous: true

must_haves:
  truths:
    - "Hydra default.yaml configures all callbacks with _target_ keys and sensible defaults"
    - "ModelCheckpoint monitors val/acc_top1 in max mode, saves top-3 and last"
    - "EarlyStopping monitors val/acc_top1 with patience=10"
    - "All callback _target_ paths resolve to importable classes"
    - "A smoke training loop with all callbacks configured completes 2 epochs without error"
  artifacts:
    - path: "src/classifier_training/conf/callbacks/default.yaml"
      provides: "Hydra callback configuration for all 12 callbacks"
      contains: "_target_"
    - path: "tests/test_callbacks_integration.py"
      provides: "Integration test: Hydra config loads and smoke training loop completes"
  key_links:
    - from: "src/classifier_training/conf/callbacks/default.yaml"
      to: "src/classifier_training/callbacks/"
      via: "Hydra _target_ references to callback classes"
      pattern: "_target_: classifier_training.callbacks"
    - from: "src/classifier_training/conf/callbacks/default.yaml"
      to: "lightning.pytorch.callbacks"
      via: "Hydra _target_ references to Lightning built-in callbacks"
      pattern: "_target_: lightning.pytorch.callbacks"
---

<objective>
Create the Hydra `conf/callbacks/default.yaml` configuration wiring all 12 callbacks (8 custom + 4 Lightning built-in) with sensible defaults, and write an integration test that loads the config and runs a 2-epoch smoke training loop.

Purpose: Wire all callbacks into a single Hydra-composable config group so that Phase 4 can reference `callbacks=default` in the training config. The integration test proves all callbacks coexist without conflicts.
Output: Hydra default.yaml for callbacks; integration test proving the full callback stack works together.
</objective>

<execution_context>
@/Users/ortizeg/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ortizeg/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-callbacks-and-onnx-export/03-RESEARCH.md
@.planning/phases/03-callbacks-and-onnx-export/03-01-SUMMARY.md
@.planning/phases/03-callbacks-and-onnx-export/03-02-SUMMARY.md

# Sibling default.yaml for reference:
@/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/code/object-detection-training/src/object_detection_training/conf/callbacks/default.yaml

# Existing conf structure:
@src/classifier_training/conf/models/resnet18.yaml
@src/classifier_training/conf/models/resnet50.yaml
@src/classifier_training/callbacks/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Hydra default.yaml for all callbacks</name>
  <files>
    src/classifier_training/conf/callbacks/default.yaml
  </files>
  <action>
1. Create `src/classifier_training/conf/callbacks/default.yaml` with flat keys (no wrapper — per Phase 1 decision about config group YAML format):

   ```yaml
   # Lightning built-in callbacks
   model_checkpoint:
     _target_: lightning.pytorch.callbacks.ModelCheckpoint
     dirpath: ${hydra:runtime.output_dir}/checkpoints
     filename: "epoch={epoch:02d}-val_acc={val/acc_top1:.4f}"
     monitor: val/acc_top1
     mode: max
     save_top_k: 3
     save_last: true

   early_stopping:
     _target_: lightning.pytorch.callbacks.EarlyStopping
     monitor: val/acc_top1
     mode: max
     patience: 10
     min_delta: 0.001

   lr_monitor:
     _target_: lightning.pytorch.callbacks.LearningRateMonitor
     logging_interval: epoch

   rich_progress:
     _target_: lightning.pytorch.callbacks.RichProgressBar

   # Custom callbacks
   ema:
     _target_: classifier_training.callbacks.ema.EMACallback
     decay: 0.9999
     warmup_steps: 2000

   onnx_export:
     _target_: classifier_training.callbacks.onnx_export.ONNXExportCallback
     output_dir: ${hydra:runtime.output_dir}
     opset_version: 17
     input_height: 224
     input_width: 224

   confusion_matrix:
     _target_: classifier_training.callbacks.confusion_matrix.ConfusionMatrixCallback
     output_dir: ${hydra:runtime.output_dir}
     num_classes: 43

   model_info:
     _target_: classifier_training.callbacks.model_info.ModelInfoCallback
     output_dir: ${hydra:runtime.output_dir}

   dataset_statistics:
     _target_: classifier_training.callbacks.statistics.DatasetStatisticsCallback

   training_history:
     _target_: classifier_training.callbacks.plotting.TrainingHistoryCallback
     output_dir: ${hydra:runtime.output_dir}

   sampler_distribution:
     _target_: classifier_training.callbacks.sampler.SamplerDistributionCallback

   sample_visualization:
     _target_: classifier_training.callbacks.visualization.SampleVisualizationCallback
     output_dir: ${hydra:runtime.output_dir}
     num_samples: 16
   ```

   This covers all 12 requirements: CALL-01 (EMA), CALL-02 (ONNX), CALL-03 (ConfusionMatrix), CALL-04 (ModelInfo), CALL-05 (DatasetStatistics), CALL-06 (TrainingHistory), CALL-07 (SampleViz), CALL-08 (SamplerDist), CALL-09 (RichProgressBar), CALL-10 (LRMonitor), CALL-11 (EarlyStopping), CALL-12 (ModelCheckpoint).
  </action>
  <verify>
    File exists at `src/classifier_training/conf/callbacks/default.yaml`.
    YAML is valid: `pixi run python -c "import yaml; yaml.safe_load(open('src/classifier_training/conf/callbacks/default.yaml'))"` succeeds.
    All 12 callback keys present in the file.
  </verify>
  <done>
    Hydra callback config created with all 12 callbacks configured with sensible defaults. ModelCheckpoint monitors val/acc_top1 max mode. EarlyStopping patience=10. EMA decay=0.9999 warmup=2000.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integration test — Hydra config loads and smoke training loop completes</name>
  <files>
    tests/test_callbacks_integration.py
  </files>
  <action>
1. Create `tests/test_callbacks_integration.py`:

   **Test 1: test_default_yaml_all_targets_importable**
   - Load `default.yaml` with `yaml.safe_load`
   - For each entry with `_target_`, verify the target class is importable via `importlib`
   - This catches typos in _target_ paths

   **Test 2: test_smoke_training_loop_with_callbacks**
   - Create a minimal setup:
     - Small model: `nn.Linear(3*32*32, 10)` wrapped in a minimal LightningModule that implements training_step, validation_step, configure_optimizers, and logs `val/acc_top1`, `val/loss`, `train/loss`
     - Synthetic dataset: `torch.randn(32, 3, 32, 32)` with `torch.randint(0, 10, (32,))` labels
     - `tmp_path` for output directory
     - Mock datamodule: create a simple object with `save_labels_mapping(save_path: Path)` that writes `{"0": "class_0", ...}` to the given path, and set `trainer.datamodule` to this mock. This ensures ONNXExportCallback and ModelInfoCallback exercise the `labels_mapping.json` write path.
   - Instantiate these callbacks manually (not via Hydra — that's Phase 4):
     - `EMACallback(decay=0.999, warmup_steps=5)`
     - `ONNXExportCallback(output_dir=str(tmp_path), opset_version=17, input_height=32, input_width=32)` — use 32x32 for speed
     - `ConfusionMatrixCallback(output_dir=str(tmp_path), num_classes=10)`
     - `TrainingHistoryCallback(output_dir=str(tmp_path))`
     - `ModelInfoCallback(output_dir=str(tmp_path))`
     - `ModelCheckpoint(dirpath=str(tmp_path / "checkpoints"), monitor="val/acc_top1", mode="max")`
     - Do NOT include SamplerDistribution or DatasetStatistics (they need real datamodule structure)
     - Do NOT include SampleVisualization (needs real image tensors with denormalization)
   - Create `L.Trainer(max_epochs=2, callbacks=[...], enable_checkpointing=True, logger=False, accelerator="cpu", devices=1, default_root_dir=str(tmp_path))`
   - Run `trainer.fit(model, train_dataloaders=train_dl, val_dataloaders=val_dl)`
   - Assert: training completes without error
   - Assert: checkpoint file exists in `tmp_path / "checkpoints"`
   - Assert: `model.onnx` file exists in `tmp_path`
   - Assert: ONNX model output name is "logits" (load with onnxruntime)
   - Assert: confusion matrix PNG exists
   - Assert: `labels_mapping.json` exists in `tmp_path` (written by ONNXExportCallback via mock datamodule)

   **Test 3: test_onnx_export_uses_ema_weights**
   - Same minimal setup but with only EMACallback + ONNXExportCallback
   - After training, load ONNX model with onnxruntime
   - Run inference with dummy input
   - Verify output shape is correct (batch_size, num_classes)
   - This is the end-to-end integration test for CALL-01 + CALL-02

   - Use `providers=["CPUExecutionProvider"]` for all onnxruntime sessions
   - Mark slow tests with `@pytest.mark.slow` if they take > 5 seconds

2. `pixi run lint && pixi run typecheck` — fix any issues.
  </action>
  <verify>
    `pixi run pytest tests/test_callbacks_integration.py -v` passes all tests.
    `pixi run pytest` passes (full suite, no regressions).
    `pixi run lint && pixi run typecheck` pass.
  </verify>
  <done>
    Integration tests prove: all callback _target_ paths are importable; a 2-epoch smoke loop with EMA + ONNX + ConfusionMatrix + ModelCheckpoint completes without error and produces expected artifacts (checkpoint, model.onnx, confusion_matrix.png). ONNX model validates under onnxruntime.
  </done>
</task>

</tasks>

<verification>
1. `pixi run pytest tests/test_callbacks_integration.py -v` — all integration tests pass
2. `pixi run pytest` — full suite passes (no regressions)
3. `pixi run lint && pixi run typecheck` — clean
4. `default.yaml` contains all 12 callback configs with valid _target_ paths
5. Smoke training produces: checkpoint, model.onnx (output "logits"), confusion_matrix.png
</verification>

<success_criteria>
- Hydra `conf/callbacks/default.yaml` has all 12 callbacks configured (CALL-01 through CALL-12)
- ModelCheckpoint monitors val/acc_top1 max mode, saves top-3 + last
- EarlyStopping monitors val/acc_top1 with patience=10
- All _target_ paths resolve to importable classes
- Smoke 2-epoch training completes producing checkpoint + model.onnx + confusion_matrix.png
- ONNX model output name is "logits", validates under onnxruntime CPUExecutionProvider
</success_criteria>

<output>
After completion, create `.planning/phases/03-callbacks-and-onnx-export/03-03-SUMMARY.md`
</output>
