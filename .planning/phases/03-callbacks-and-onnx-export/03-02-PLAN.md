---
phase: 03-callbacks-and-onnx-export
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/classifier_training/callbacks/confusion_matrix.py
  - src/classifier_training/callbacks/statistics.py
  - src/classifier_training/callbacks/model_info.py
  - src/classifier_training/callbacks/plotting.py
  - src/classifier_training/callbacks/visualization.py
  - src/classifier_training/callbacks/sampler.py
  - src/classifier_training/callbacks/__init__.py
  - tests/test_callbacks_observability.py
autonomous: true

must_haves:
  truths:
    - "ConfusionMatrixCallback produces per-epoch PNG heatmaps during validation without GPU device errors"
    - "DatasetStatisticsCallback prints class distribution table at training start using rich"
    - "ModelInfoCallback computes parameter count, model size, and writes labels_mapping.json sidecar"
    - "TrainingHistoryCallback saves loss/accuracy curve PNGs per epoch"
    - "SamplerDistributionCallback logs class sample counts from TrackingWeightedRandomSampler"
    - "SampleVisualizationCallback saves predicted-vs-true overlay images to disk"
  artifacts:
    - path: "src/classifier_training/callbacks/confusion_matrix.py"
      provides: "ConfusionMatrixCallback"
      contains: "MulticlassConfusionMatrix"
    - path: "src/classifier_training/callbacks/statistics.py"
      provides: "DatasetStatisticsCallback"
      contains: "class DatasetStatisticsCallback"
    - path: "src/classifier_training/callbacks/model_info.py"
      provides: "ModelInfoCallback"
      contains: "class ModelInfoCallback"
    - path: "src/classifier_training/callbacks/plotting.py"
      provides: "TrainingHistoryCallback"
      contains: "class TrainingHistoryCallback"
    - path: "src/classifier_training/callbacks/visualization.py"
      provides: "SampleVisualizationCallback"
      contains: "class SampleVisualizationCallback"
    - path: "src/classifier_training/callbacks/sampler.py"
      provides: "SamplerDistributionCallback"
      contains: "class SamplerDistributionCallback"
    - path: "tests/test_callbacks_observability.py"
      provides: "Tests for all six observability callbacks"
  key_links:
    - from: "src/classifier_training/callbacks/confusion_matrix.py"
      to: "torchmetrics.classification.MulticlassConfusionMatrix"
      via: "import and device-aware initialization in on_fit_start"
      pattern: "MulticlassConfusionMatrix.*to.*device"
    - from: "src/classifier_training/callbacks/sampler.py"
      to: "src/classifier_training/data/sampler.py"
      via: "TrackingWeightedRandomSampler._last_indices access"
      pattern: "_last_indices"
---

<objective>
Implement all six observability callbacks: ConfusionMatrixCallback (new), DatasetStatisticsCallback, ModelInfoCallback, TrainingHistoryCallback, SamplerDistributionCallback, and SampleVisualizationCallback.

Purpose: Provide full training observability — developers can see class distributions, model architecture stats, per-epoch confusion matrices, accuracy curves, sampler balance verification, and sample predictions without any external logging service.
Output: Six callback classes with tests, all saving artifacts to disk (no WandB — that's Phase 4).
</objective>

<execution_context>
@/Users/ortizeg/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ortizeg/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-callbacks-and-onnx-export/03-RESEARCH.md
@.planning/phases/03-callbacks-and-onnx-export/03-01-SUMMARY.md

# Sibling source files to port from:
@/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/code/object-detection-training/src/object_detection_training/callbacks/model_info.py
@/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/code/object-detection-training/src/object_detection_training/callbacks/statistics.py
@/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/code/object-detection-training/src/object_detection_training/callbacks/plotting.py
@/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/code/object-detection-training/src/object_detection_training/callbacks/visualization.py
@/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/code/object-detection-training/src/object_detection_training/callbacks/sampler_distribution.py

# Existing classifier-training code:
@src/classifier_training/types.py
@src/classifier_training/models/base.py
@src/classifier_training/data/datamodule.py
@src/classifier_training/callbacks/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement ConfusionMatrix, DatasetStatistics, and ModelInfo callbacks</name>
  <files>
    src/classifier_training/callbacks/confusion_matrix.py
    src/classifier_training/callbacks/statistics.py
    src/classifier_training/callbacks/model_info.py
  </files>
  <action>
1. Create `src/classifier_training/callbacks/confusion_matrix.py` (NEW — no sibling equivalent):
   - Class `ConfusionMatrixCallback(L.Callback)` with `__init__(self, output_dir: str = "outputs", num_classes: int = 43)`
   - `on_fit_start`: initialize `MulticlassConfusionMatrix(num_classes=num_classes).to(pl_module.device)` — MUST be in on_fit_start, not __init__, to get correct device
   - `on_validation_batch_end`: call `self._cm.update(preds, labels)` where preds = `logits.argmax(dim=1)`
   - Get logits from `outputs` dict if available (Lightning passes step return value), otherwise run `pl_module(images)` on the batch
   - `on_validation_epoch_end`: compute confusion matrix, call `.cpu()` before passing to matplotlib, reset metric, save PNG
   - `_plot_and_save`: use `matplotlib.use("Agg")` inside method (not global), `imshow` with "Blues" colormap, save to `{output_dir}/epoch_{epoch:03d}/confusion_matrix.png`
   - Add class labels on axes if `class_names` is provided (optional `class_names: list[str] | None = None` param)
   - Close figure with `plt.close(fig)` to prevent memory leak

2. Create `src/classifier_training/callbacks/statistics.py` — simplified port from sibling:
   - Class `DatasetStatisticsCallback(L.Callback)` — no __init__ params needed
   - `on_fit_start`: access `trainer.datamodule._train_dataset.samples` to get `list[tuple[str, int]]`
   - Count labels per class, build `idx_to_class` from `trainer.datamodule.class_to_idx`
   - Print `rich.table.Table` with columns: Class Name, Index, Count, Percentage
   - Sort by index for consistent display
   - Also log total samples count with loguru
   - Handle case where datamodule or _train_dataset is None (log warning and return)

3. Create `src/classifier_training/callbacks/model_info.py` — port from sibling, simplified:
   - Class `ModelInfoCallback(L.Callback)` with `__init__(self, output_dir: str = "outputs")`
   - `on_fit_start`: compute basic stats — total parameters, trainable parameters, model size in MB
   - Parameter count: `sum(p.numel() for p in model.parameters())`
   - Trainable: `sum(p.numel() for p in model.parameters() if p.requires_grad)`
   - Size: `sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)`
   - Print rich table with model info
   - Write `labels_mapping.json` to output_dir by calling `trainer.datamodule.save_labels_mapping(Path(self.output_dir))` if datamodule available
   - Do NOT use fvcore for FLOPs — basic parameter counting only (per research decision)
  </action>
  <verify>
    `pixi run python -c "from classifier_training.callbacks.confusion_matrix import ConfusionMatrixCallback; print('OK')"` succeeds.
    `pixi run python -c "from classifier_training.callbacks.statistics import DatasetStatisticsCallback; print('OK')"` succeeds.
    `pixi run python -c "from classifier_training.callbacks.model_info import ModelInfoCallback; print('OK')"` succeeds.
    `pixi run lint` passes.
  </verify>
  <done>
    ConfusionMatrixCallback produces per-epoch PNG heatmaps with correct device handling. DatasetStatisticsCallback prints class distribution via rich table. ModelInfoCallback reports parameter counts and writes labels_mapping.json.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement TrainingHistory, SamplerDistribution, SampleVisualization callbacks + all tests</name>
  <files>
    src/classifier_training/callbacks/plotting.py
    src/classifier_training/callbacks/sampler.py
    src/classifier_training/callbacks/visualization.py
    src/classifier_training/callbacks/__init__.py
    tests/test_callbacks_observability.py
  </files>
  <action>
1. Create `src/classifier_training/callbacks/plotting.py` — port from sibling, adapted metrics:
   - Class `TrainingHistoryCallback(L.Callback)` with `__init__(self, output_dir: str = "outputs")`
   - Track metrics: `train/loss`, `val/loss`, `val/acc_top1`, `val/acc_top5`, `train/acc_top1`
   - `on_train_epoch_end`: read metrics from `trainer.callback_metrics` dict, append to internal history lists
   - `on_validation_epoch_end`: save plot PNGs (loss curves + accuracy curves) to `{output_dir}/training_history/`
   - Two subplots: (1) train_loss + val_loss, (2) val_acc_top1 + val_acc_top5
   - Use `matplotlib.use("Agg")` inside plot method, `plt.close(fig)` after save

2. Create `src/classifier_training/callbacks/sampler.py` — port from sibling sampler_distribution.py:
   - Class `SamplerDistributionCallback(L.Callback)` — no __init__ params
   - `on_train_epoch_start`: access `trainer.train_dataloader.sampler` and check for `_last_indices` attribute (indicates `TrackingWeightedRandomSampler`)
   - If `_last_indices` available: map indices to class labels using `trainer.datamodule._train_dataset.samples`, count per-class, log with loguru
   - Print rich table: Class Name, Sampled Count, Expected Count, Ratio
   - Handle case where sampler is not TrackingWeightedRandomSampler (log info and skip)
   - Note: `_last_indices` is populated AFTER the first epoch's iteration completes, so this fires from epoch 1 onward (epoch 0 has empty indices)

3. Create `src/classifier_training/callbacks/visualization.py` — new for classification (sibling is detection-specific):
   - Class `SampleVisualizationCallback(L.Callback)` with `__init__(self, output_dir: str = "outputs", num_samples: int = 16)`
   - `on_validation_batch_end`: collect first N samples from validation (store images + true labels + predicted labels)
   - `on_validation_epoch_end`: create a grid of sample images with "True: X / Pred: Y" text overlay
   - Use `matplotlib` for the grid (not torchvision.utils.make_grid — easier text overlay)
   - Denormalize images using IMAGENET_MEAN/IMAGENET_STD before display
   - Save to `{output_dir}/epoch_{epoch:03d}/sample_predictions.png`
   - Color-code: green border/text for correct, red for incorrect

4. Update `src/classifier_training/callbacks/__init__.py`:
   - Export all 8 callback classes (2 from plan 03-01 + 6 new)

5. Create `tests/test_callbacks_observability.py`:
   - Test ConfusionMatrixCallback: verify PNG is created after simulated validation epoch, verify device handling (create metric on CPU, pass CPU tensors)
   - Test DatasetStatisticsCallback: mock trainer.datamodule with samples list, verify it runs without error (output goes to rich console)
   - Test ModelInfoCallback: verify parameter count is computed correctly for a small model, verify labels_mapping.json is written
   - Test TrainingHistoryCallback: mock callback_metrics, verify PNG files created after epoch
   - Test SamplerDistributionCallback: create TrackingWeightedRandomSampler with known indices, verify callback reads them correctly
   - Test SampleVisualizationCallback: verify PNG grid is created with correct number of samples
   - All tests use minimal models (nn.Linear or small custom), NOT ResNet
   - All tests use `tmp_path` fixture for output directories

6. `pixi run lint && pixi run typecheck` — fix any issues.
  </action>
  <verify>
    `pixi run pytest tests/test_callbacks_observability.py -v` passes all tests.
    `pixi run pytest` passes (all existing tests still pass).
    `pixi run lint && pixi run typecheck` pass.
  </verify>
  <done>
    All six observability callbacks implemented and tested. Each saves artifacts to disk without WandB dependency. ConfusionMatrix handles device correctly. SamplerDistribution reads TrackingWeightedRandomSampler indices. DatasetStatistics prints rich table at training start.
  </done>
</task>

</tasks>

<verification>
1. `pixi run pytest tests/test_callbacks_observability.py -v` — all observability callback tests pass
2. `pixi run pytest` — full suite passes (no regressions)
3. `pixi run lint && pixi run typecheck` — clean
4. All 6 callback classes importable from `classifier_training.callbacks`
</verification>

<success_criteria>
- ConfusionMatrixCallback produces PNG heatmap, initializes torchmetrics on correct device in on_fit_start
- DatasetStatisticsCallback prints class distribution rich table at training start
- ModelInfoCallback reports parameters/size and writes labels_mapping.json
- TrainingHistoryCallback saves loss/accuracy curve PNGs
- SamplerDistributionCallback reads TrackingWeightedRandomSampler._last_indices and logs class counts
- SampleVisualizationCallback saves predicted-vs-true grid PNG
- All callbacks save to disk only (no WandB)
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/03-callbacks-and-onnx-export/03-02-SUMMARY.md`
</output>
