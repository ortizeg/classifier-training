---
phase: 04-training-configuration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pixi.toml
  - src/classifier_training/train.py
  - src/classifier_training/conf/train_basketball_resnet18.yaml
  - src/classifier_training/conf/trainer/default.yaml
  - src/classifier_training/conf/data/basketball_jersey_numbers.yaml
  - src/classifier_training/conf/logging/wandb.yaml
  - src/classifier_training/conf/callbacks/default.yaml
  - src/classifier_training/data/datamodule.py
autonomous: true

must_haves:
  truths:
    - "Hydra config composes without error from root config with all 5 groups (model, data, trainer, callbacks, logging)"
    - "ImageFolderDataModule can be instantiated via hydra.utils.instantiate with flat kwargs"
    - "train.py entrypoint is callable via python -m classifier_training.train"
    - "ModelCheckpoint dirpath is fixed (not Hydra-timestamped) for resume stability"
    - "wandb is installed in pixi environment"
  artifacts:
    - path: "src/classifier_training/train.py"
      provides: "@hydra.main entrypoint that instantiates model, data, callbacks, loggers, trainer"
    - path: "src/classifier_training/conf/train_basketball_resnet18.yaml"
      provides: "Root Hydra config with defaults list selecting all config groups"
    - path: "src/classifier_training/conf/trainer/default.yaml"
      provides: "T4-tuned trainer config (16-mixed, clip_val=1.0, accumulate_grad_batches=1)"
    - path: "src/classifier_training/conf/data/basketball_jersey_numbers.yaml"
      provides: "Dataset config for basketball-jersey-numbers-ocr (batch_size=64, num_workers=4)"
    - path: "src/classifier_training/conf/logging/wandb.yaml"
      provides: "WandbLogger configuration"
  key_links:
    - from: "src/classifier_training/train.py"
      to: "src/classifier_training/conf/train_basketball_resnet18.yaml"
      via: "@hydra.main(config_name='train_basketball_resnet18')"
      pattern: "config_name.*train_basketball_resnet18"
    - from: "src/classifier_training/train.py"
      to: "src/classifier_training/data/datamodule.py"
      via: "hydra.utils.instantiate(cfg.data)"
      pattern: "instantiate.*cfg\\.data"
    - from: "src/classifier_training/conf/callbacks/default.yaml"
      to: "lightning.pytorch.callbacks.ModelCheckpoint"
      via: "fixed dirpath for resume stability"
      pattern: "dirpath:.*checkpoints"
---

<objective>
Wire the completed codebase (Phases 1-3) into a launchable Hydra training pipeline: create all config group YAMLs, the train.py entrypoint, adapt ImageFolderDataModule for Hydra instantiation, fix ModelCheckpoint dirpath for resume stability, and add wandb + train task to pixi.toml.

Purpose: This is the core wiring plan — after this, `pixi run train` can launch a training run with all config groups composing correctly.
Output: train.py entrypoint, 4 new config YAMLs, modified DataModule and callbacks config, updated pixi.toml.
</objective>

<execution_context>
@/Users/ortizeg/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ortizeg/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-training-configuration/04-RESEARCH.md

# Prior phase summaries needed for DataModule signature + callback structure
@.planning/phases/01-foundation-and-data-pipeline/01-03-SUMMARY.md
@.planning/phases/03-callbacks-and-onnx-export/03-03-SUMMARY.md

# Key source files to understand existing contracts
@src/classifier_training/data/datamodule.py
@src/classifier_training/config.py
@src/classifier_training/conf/callbacks/default.yaml
@src/classifier_training/conf/models/resnet18.yaml
@src/classifier_training/utils/hydra.py
@pixi.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Hydra config YAMLs + pixi.toml dependencies</name>
  <files>
    pixi.toml
    src/classifier_training/conf/train_basketball_resnet18.yaml
    src/classifier_training/conf/trainer/default.yaml
    src/classifier_training/conf/data/basketball_jersey_numbers.yaml
    src/classifier_training/conf/logging/wandb.yaml
    src/classifier_training/conf/callbacks/default.yaml
  </files>
  <action>
**1. Add wandb dependency and train task to pixi.toml:**

In `[dependencies]` section, add:
```
wandb = ">=0.23.1,<0.24"
```

In `[tasks]` section, add:
```
train = "python -m classifier_training.train"
```

Run `pixi install` to resolve the environment.

**2. Create `src/classifier_training/conf/trainer/default.yaml`:**

No `_target_` key — Trainer is constructed directly in train.py, not via hydra.utils.instantiate. Flat keys matching L.Trainer constructor parameters:

```yaml
max_epochs: 100
precision: 16-mixed
accelerator: auto
devices: auto
strategy: auto
val_check_interval: 1.0
log_every_n_steps: 10
enable_checkpointing: true
enable_progress_bar: true
enable_model_summary: true
gradient_clip_val: 1.0
gradient_clip_algorithm: norm
accumulate_grad_batches: 1
num_sanity_val_steps: 2
```

**3. Create `src/classifier_training/conf/data/basketball_jersey_numbers.yaml`:**

```yaml
_target_: classifier_training.data.datamodule.ImageFolderDataModule
data_root: /Users/ortizeg/1Projects/⛹️‍♂️ Next Play/data/basketball-jersey-numbers-ocr
batch_size: 64
num_workers: 4
pin_memory: true
persistent_workers: true
image_size: 224
```

**4. Create `src/classifier_training/conf/logging/wandb.yaml`:**

```yaml
wandb:
  _target_: lightning.pytorch.loggers.WandbLogger
  project: classifier-training
  name: null
  save_dir: ${hydra:runtime.output_dir}
  log_model: false
  tags: []
  notes: null
```

**5. Create `src/classifier_training/conf/train_basketball_resnet18.yaml`:**

```yaml
defaults:
  - _self_
  - model: resnet18
  - data: basketball_jersey_numbers
  - trainer: default
  - callbacks: default
  - logging: wandb

seed: 42
log_level: INFO

hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: outputs/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
```

**CRITICAL: Verify the config group directory names match the defaults list keys.** The existing `conf/models/` directory is referenced as `model: resnet18` in the defaults list. This works because Hydra maps the config group name `model` to the directory `models/` — NO, actually Hydra requires exact directory name match. Check if the existing directory is `models/` (plural) or `model/` (singular). If it's `models/`, use `models: resnet18` in the defaults list OR rename the directory to `model/`. The existing `@register` decorator in `utils/hydra.py` uses `group_name` derived from the module path — check what group name it registers and ensure consistency.

Actually, re-read `src/classifier_training/utils/hydra.py` to confirm the ConfigStore group name. The defaults list key MUST match the directory name exactly. If directory is `models/`, defaults list must say `models: resnet18`. If directory is `model/`, defaults list must say `model: resnet18`.

Similarly, `callbacks/` directory exists already — the defaults list should say `callbacks: default`.

**6. Fix ModelCheckpoint dirpath in `src/classifier_training/conf/callbacks/default.yaml`:**

Change line 4 from:
```yaml
  dirpath: ${hydra:runtime.output_dir}/checkpoints
```
to:
```yaml
  dirpath: checkpoints
```

This is a relative path that Lightning resolves against `Trainer.default_root_dir`, which train.py will set to a fixed project-level directory. This ensures `ckpt_path="last"` can find the previous run's checkpoint (TRAIN-07).

Also update ONNX export and confusion matrix `output_dir` to NOT use `${hydra:runtime.output_dir}` if resume stability requires it — but these are output artifacts (not resume inputs), so timestamped dirs are fine for them.
  </action>
  <verify>
1. `pixi install` completes without error
2. `pixi run python -c "import wandb; print(wandb.__version__)"` prints version
3. All 6 YAML files exist and are valid YAML: `pixi run python -c "from omegaconf import OmegaConf; OmegaConf.load('src/classifier_training/conf/train_basketball_resnet18.yaml')"`
4. `pixi run python -c "from hydra import compose, initialize_config_dir; import os; initialize_config_dir(config_dir=os.path.abspath('src/classifier_training/conf'), version_base=None); cfg = compose(config_name='train_basketball_resnet18'); print(cfg.trainer.precision)"` prints `16-mixed`
  </verify>
  <done>
All config group YAMLs exist (trainer/default.yaml, data/basketball_jersey_numbers.yaml, logging/wandb.yaml, root config). wandb is installed. `pixi run train` task is defined. ModelCheckpoint dirpath is fixed (not timestamped). Hydra config composes from root config without error.
  </done>
</task>

<task type="auto">
  <name>Task 2: DataModule Hydra adapter + train.py entrypoint</name>
  <files>
    src/classifier_training/data/datamodule.py
    src/classifier_training/train.py
  </files>
  <action>
**1. Modify `ImageFolderDataModule.__init__` for Hydra compatibility:**

The current signature `__init__(self, config: DataModuleConfig)` is incompatible with `hydra.utils.instantiate(cfg.data)` which passes flat kwargs. Change to accept BOTH patterns — a `config` keyword argument OR individual kwargs:

```python
def __init__(
    self,
    config: DataModuleConfig | None = None,
    *,
    data_root: str = "",
    batch_size: int = 32,
    num_workers: int = 4,
    pin_memory: bool = True,
    persistent_workers: bool = True,
    image_size: int = 224,
    **kwargs: Any,
) -> None:
    super().__init__()
    if config is not None:
        self._config = config
    else:
        self._config = DataModuleConfig(
            data_root=data_root,
            batch_size=batch_size,
            num_workers=num_workers,
            pin_memory=pin_memory,
            persistent_workers=persistent_workers,
            image_size=image_size,
        )
    # ... rest of __init__ stays the same, using self._config
```

The `**kwargs: Any` absorbs any extra Hydra-injected keys (like `_target_`, `_recursive_`, etc.) that `hydra.utils.instantiate` may pass. Import `Any` from `typing`.

IMPORTANT: All existing tests call `ImageFolderDataModule(cfg)` where `cfg` is a `DataModuleConfig` — this still works because `config` is the first positional parameter. No test changes needed.

**2. Create `src/classifier_training/train.py`:**

```python
"""Training entrypoint for classifier_training.

Usage:
    pixi run train                              # defaults
    pixi run train model=resnet50               # override model
    pixi run train data.batch_size=32           # override batch size
    pixi run train trainer.max_epochs=10        # override epochs
"""

import sys
from typing import Any

import hydra
import lightning as L
from hydra.core.hydra_config import HydraConfig
from loguru import logger
from omegaconf import DictConfig, OmegaConf

# CRITICAL: import models to trigger @register decorators BEFORE Hydra parses config
import classifier_training.models  # noqa: F401


@hydra.main(version_base=None, config_path="conf", config_name="train_basketball_resnet18")
def main(cfg: DictConfig) -> None:
    """Run training with the given Hydra config."""
    # Setup logging
    logger.remove()
    logger.add(sys.stderr, level=cfg.get("log_level", "INFO"))

    logger.info(f"Configuration:\n{OmegaConf.to_yaml(cfg)}")

    # Seed everything for reproducibility
    L.seed_everything(cfg.get("seed", 42), workers=True)

    # Instantiate data module
    datamodule: L.LightningDataModule = hydra.utils.instantiate(cfg.data)
    datamodule.setup("fit")

    # Instantiate model and set class weights from data
    model: L.LightningModule = hydra.utils.instantiate(cfg.model)
    model.set_class_weights(datamodule.get_class_weights())  # type: ignore[attr-defined]

    # Instantiate callbacks
    callbacks: list[L.Callback] = []
    if cfg.get("callbacks"):
        for v in cfg.callbacks.values():
            if v is not None and "_target_" in v:
                callbacks.append(hydra.utils.instantiate(v))

    # Instantiate loggers
    loggers: list[Any] = []
    if cfg.get("logging"):
        for v in cfg.logging.values():
            if v is not None and "_target_" in v:
                loggers.append(hydra.utils.instantiate(v))

    # Build Trainer from config dict (NOT via hydra.utils.instantiate —
    # trainer config has no _target_ key)
    trainer_cfg = dict(cfg.trainer)
    trainer = L.Trainer(
        **trainer_cfg,
        callbacks=callbacks,
        logger=loggers or False,
        default_root_dir=HydraConfig.get().runtime.cwd,
    )

    # Train! ckpt_path="last" enables automatic resume from last checkpoint
    trainer.fit(model, datamodule=datamodule, ckpt_path="last")


if __name__ == "__main__":
    main()
```

Key design decisions:
- `default_root_dir=HydraConfig.get().runtime.cwd` sets the root to the original working directory (fixed across runs), so `ModelCheckpoint(dirpath="checkpoints")` resolves to `{cwd}/checkpoints` — stable for resume.
- `ckpt_path="last"` (TRAIN-07): Lightning auto-finds `last.ckpt` in the checkpoint dir. If no checkpoint exists, starts from scratch.
- `logger=loggers or False`: When no loggers configured, pass `False` to disable default TensorBoard logger.
- `import classifier_training.models` before `@hydra.main` ensures ConfigStore is populated.
- `model.set_class_weights(datamodule.get_class_weights())` bridges the class imbalance handling from Phase 1 to the model loss.
  </action>
  <verify>
1. `pixi run lint` passes (no ruff violations in train.py or datamodule.py)
2. `pixi run typecheck` passes (mypy on src/)
3. All existing tests still pass: `pixi run test` (90 tests)
4. `pixi run python -c "from classifier_training.train import main; print('OK')"` succeeds
  </verify>
  <done>
ImageFolderDataModule accepts both `config=DataModuleConfig(...)` (existing pattern) and flat kwargs from Hydra (new pattern). train.py entrypoint is created with @hydra.main, model/data/callback/logger instantiation, fixed default_root_dir, and ckpt_path="last" for resume. All existing 90 tests still pass.
  </done>
</task>

</tasks>

<verification>
1. `pixi install` succeeds — wandb resolved in environment
2. `pixi run test` — all 90 existing tests pass (DataModule adapter is backward compatible)
3. `pixi run lint && pixi run typecheck` — clean on all modified/new files
4. Hydra config composition: `pixi run python -c "from hydra import compose, initialize_config_dir; ..."` resolves all defaults without error
5. ModelCheckpoint dirpath is `checkpoints` (not `${hydra:runtime.output_dir}/checkpoints`)
</verification>

<success_criteria>
- All 5 config group YAMLs exist and compose from root config
- ImageFolderDataModule instantiable via hydra.utils.instantiate(cfg.data)
- train.py exists and is importable
- wandb is installed
- `pixi run train` task defined in pixi.toml
- 90 existing tests still pass
</success_criteria>

<output>
After completion, create `.planning/phases/04-training-configuration/04-01-SUMMARY.md`
</output>
